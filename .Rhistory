ggplot(data.table(mse.bss),aes(y=mse.bss,x=seq(1:13)))+geom_line()
which.min(mse.bss)
ridge_info
lasso_info
pcr_info
summary(pcr.fit)
ridge_info
summary(ridge.model)
lasso_info
summary(lasso.model)
pcr_info
summary(pcr.fit)
library(ISLR)
library(knitr)
library(broom)
library(GGally)
library(kableExtra)
library(ISLR)
library(caret)
library(tidyverse)
library(fastICA)
data('College')
set.seed(1)
tr <- createDataPartition(College$Apps, p = 0.75, list = FALSE)
train <- College[tr,]
test <- College[-tr,]
preObj <- preProcess(train, method = c('center', 'scale'))
train <- predict(preObj, train)
test <- predict(preObj, test)
y_train <- train$Apps
y_test <- test$Apps
dummies <- dummyVars(Apps ~ ., data = train)
x_train <- predict(dummies, train)
x_test <- predict(dummies, test)
linear <- lm(Apps ~ ., data = train)
ggplotRegression(linear)
pred_try <- predict(linear, test)
linear_info <- postResample(pred_try, test$Apps)
linear_info
ridge <- train(x = x_train, y = y_train,
method = 'glmnet',
trControl = trainControl(method = 'cv', number = 10),
tuneGrid = expand.grid(alpha = 0,
lambda = seq(0, 10e2, length.out = 20)))
ridge_info <- postResample(predict(ridge, x_test), y_test)
#RMSE and R^2
ridge_info
plot(ridge, main = "Ridge Regression Plot")
plot(varImp(ridge), main = "Variable Importance From Ridge Regression")
#coefficients for the best model by lambda
#ridge$finalModel
coef(ridge$finalModel, ridge$bestTune$lambda)
lasso <- train(x = x_train, y = y_train,
method = 'glmnet',
trControl = trainControl(method = 'cv', number = 10),
tuneGrid = expand.grid(alpha = 1,
lambda = seq(0.0001, 1, length.out = 50)))
lasso_info <- postResample(predict(lasso, x_test), y_test)
lasso_info
plot(lasso, main = "Lasso Regression Plot")
plot(varImp(lasso), main = "Variable Importance From Lasso Regression")
#coef from Lasso by lambda
coef(lasso$finalModel, lasso$bestTune$lambda)
pcr <- train(x = x_train, y = y_train,
method = 'pcr',
trControl = trainControl(method = 'cv', number = 10),
tuneGrid = expand.grid(ncomp = 1:10))
pcr_info <- postResample(predict(pcr, x_test), y_test)
pcr_info
plot(pcr, main = "PCR Regression Plot")
plot(varImp(pcr), main = "Variable Importance From PCR Regression")
#coef from Lasso by lambda
coef(pcr$finalModel, pcr$bestTune$lambda)
pls <- train(x = x_train, y = y_train,
method = 'pls',
trControl = trainControl(method = 'cv', number = 10),
tuneGrid = expand.grid(ncomp = 1:10))
pls_info <- postResample(predict(pls, x_test), y_test)
pls_info
plot(pls, main = "PLS Regression Plot")
plot(varImp(pls), main = "Variable Importance From PLS Regression")
#coef fplsrom Lasso by lambda
coef(lasso$finalModel, pls$bestTune$lambda)
as_data_frame(rbind(ridge_info,lasso_info,pcr_info,pls_info)) %>%
mutate(model = c('Linear', 'Ridge', 'Lasso', 'PCR', 'PLS')) %>%
select(model, RMSE, Rsquared)
sd(test$Apps) #standard deviation of Apps
mean(test$Apps) #mean of Apps
as_data_frame(rbind(ridge_info,lasso_info,pcr_info,pls_info)) %>%
mutate(model = c('Linear', 'Ridge', 'Lasso', 'PCR', 'PLS')) %>%
select(model, RMSE, Rsquared)
models = as_data_frame(rbind(ridge_info,lasso_info,pcr_info,pls_info)) %>%
mutate(model = c('Linear', 'Ridge', 'Lasso', 'PCR', 'PLS')) %>%
select(model, RMSE, Rsquared)
models = as_data_frame(rbind(ridge_info,lasso_info,pcr_info,pls_info)) %>%
mutate(model = c('Ridge', 'Lasso', 'PCR', 'PLS')) %>%
select(model, RMSE, Rsquared)
models = as_data_frame(rbind(ridge_info,lasso_info,pcr_info,pls_info)) %>%
mutate(model = c('Ridge', 'Lasso', 'PCR', 'PLS')) %>%
select(model, RMSE, Rsquared)
models = as_data_frame(rbind(ridge_info,lasso_info,pcr_info,pls_info)) %>%
mutate(model = c('Ridge', 'Lasso', 'PCR', 'PLS'))
#%>% select(model, RMSE, Rsquared)
sd(test$Apps) #standard deviation of Apps
mean(test$Apps) #mean of Apps
data(Boston)
#function to make plot with fancy labels
ggplotRegression <- function (fit) {
require(ggplot2)
ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) +
geom_point() +
stat_smooth(method = "lm", col = "blue") +
labs(title = paste("Adj R2 = ",signif(summary(fit)$adj.r.squared, 5),
"Intercept =",signif(fit$coef[[1]],5 ),
" Slope =",signif(fit$coef[[2]], 5),
" P =",signif(summary(fit)$coef[2,4], 5)))
}
# ZN VS CRIM
simreg_zm <- lm(crim ~ zn, data = Boston)
summary(simreg_zm)
ggplotRegression(simreg_zm)
summary(simreg_zm)$r.squared
knitr::opts_chunk$set(echo = TRUE)
ind1 <- which(train$Nbhd == 1)
ind2 <- which(train$Nbhd == 2)
ind <- c(ind1,ind2)
train.2<- train[-ind,]
lin.model.b <- lm(log(Price)~ Brick, data=train.2)
midcity <- read.csv("~/Documents/UT MSBA 2020-21/Summer /PredictiveModeling/MidCity.csv")
library(MASS)
library(ggplot2)
set.seed(123)
row.number <- sample(1:nrow(midcity), 0.75*nrow(midcity))
train = midcity[row.number,]
test = midcity[-row.number,]
lin.model <- lm(log(Price)~ Brick, data=train)
summary(lin.model)
par(mfrow=c(2,2))
plot(lin.model)
lin.model.mul <- lm(log(Price)~ ., data=train)
summary(lin.model.mul)
par(mfrow=c(2,2))
plot(lin.model.mul)
lin.model.mul <- update(lin.model.mul, ~.-Home)
summary(lin.model.mul)
par(mfrow=c(2,2))
plot(lin.model.mul)
library(gridExtra)
residplot = ggplot(train, aes(Price, residuals(lin.model.mul))) + geom_point() + geom_smooth()
residplot
View(midcity)
housingData$NType = ifelse(housingData$Nbhd == 3, 1,0)
housingData$NType = ifelse(midcity$Nbhd == 3, 1,0)
midcity$NType = ifelse(midcity$Nbhd == 3, 1,0)
lin.nbhd.model <- lm(Price ~ NType + Offers + SqFt + Brick + Bedrooms + Bathrooms,data=housingData )
lin.nbhd.model <- lm(Price ~ NType + Offers + SqFt + Brick + Bedrooms + Bathrooms,data=midcity )
summary(lin.nbhd.model)
midcity$brick = ifelse(midcity$Brick == "Yes", 1,0)
lin.model.mul <- lm(Price~ Nbhd+Offers+SqFt+brick+Bedrooms+Bathrooms, data=train)
summary(lin.model.mul)
par(mfrow=c(2,2))
plot(lin.model.mul)
plot(lin.model.mul)
summary(lin.model.mul)
lin.model <- lm(Price~ Brick, data=train)
summary(lin.model)
summary(lin.model.mul)$coefficients[5,1]
midcity$NType = ifelse(midcity$Nbhd == 3, 1,0)
lin.nbhd.model <- lm(Price ~ NType + Offers + SqFt + Brick + Bedrooms + Bathrooms,data=midcity )
summary(lin.nbhd.model)
lin.model.b <- lm(Price~ brick + NType, data=midcity)
summary(lin.model.b)
par(mfrow=c(2,2))
plot(lin.model.b)
housingData$brickNbd3 = ifelse((housingData$Nbhd==3 & housingData$Brick==1),1,0)
midcity$BN3 = ifelse((midcity$Nbhd==3 & midcity$Brick==1),1,0)
lin.model.b <- lm(Price~ brick + NType, data=midcity)
summary(lin.model.b)
par(mfrow=c(2,2))
plot(lin.model.b)
midcity$BN3 = ifelse((midcity$Nbhd==3 & midcity$brick==1),1,0)
# Multiple Linear Regression
lin.bn3.model <- lm(Price ~  BN3+Offers+SqFt+Bedrooms+Bathrooms,data=midcity)
summary(lin.bn3.model)
lin.model <- lm(Price ~ Brick, data=midcity)
midcity$brick = ifelse(midcity$Brick == "Yes", 1,0)
lin.model.mul <- lm(Price~ Nbhd+Offers+SqFt+brick+Bedrooms+Bathrooms, data=train)
midcity$brick = ifelse(midcity$Brick == "Yes", 1,0)
lin.model.mul <- lm(Price~ Nbhd+Offers+SqFt+brick+Bedrooms+Bathrooms, data=midcity)
summary(lin.model.mul)
par(mfrow=c(2,2))
midcity$brick = ifelse(midcity$Brick == "Yes", 1,0)
lin.model.mul <- lm(Price~ Nbhd+Offers+SqFt+brick+Bedrooms+Bathrooms, data=midcity)
summary(lin.model.mul)
par(mfrow=c(2,2))
plot(lin.model.mul)
summary(lin.model.mul)$coefficients[5,1]
library(gridExtra)
residplot = ggplot(train, aes(Price, residuals(lin.model.mul))) + geom_point() + geom_smooth()
residplot
residplot
midcity$brick = ifelse(midcity$Brick == "Yes", 1,0)
lin.model.mul <- lm(Price~ Nbhd+Offers+SqFt+brick+Bedrooms+Bathrooms, data=midcity)
summary(lin.model.mul)
par(mfrow=c(2,2))
plot(lin.model.mul)
summary(lin.model.mul)$coefficients[5,1]
library(gridExtra)
residplot = ggplot(midcity, aes(Price, residuals(lin.model.mul))) + geom_point() + geom_smooth()
##################################################################
articles <- c(df$article)
library(tidyverse)
df <- read.csv("author_train.csv")
##################################################################
articles <- c(df$article)
df$sentence.count <- 0
##################################################################
articles <- c(df$article)
##################################################################
articles <- c(df$article)
##################################################################
articles <- c(df$article)
articles <- c(df$article)
df$article
df <- read.csv("author_train.csv")
setwd("~/Documents/UT MSBA 2020-21/Summer /PredictiveModeling/Part2_Exercices/STA380Exercises")
df <- read.csv("author_train.csv")
articles <- c(df$article)
df$sentence.count <- 0
df$word.count <- 0
cnt <- 0
for (x in 1:length(articles)){
sentence<- str_replace_all(articles[x],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
SENTENCE<- strsplit(sentence,".",fixed=TRUE)
for (y in 1:length(SENTENCE)){
s[y] <- nchar(strsplit(SENTENCE[y]," ",fixed=TRUE))
df$avg_sentencelength[x] <- mean(s)
}#end of for avg sentence length
WORDS<- str_split(sentence," ")
df$word.count[x] <- nchar(WORDS)
}
sentence<- str_replace_all(articles[3],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
sentence<- strsplit(sentence,".",fixed=TRUE)
sentence<- strsplit(sentence," ",fixed=TRUE)
sentence<- str_replace_all(articles[3],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
#sentence<- strsplit(sentence,".",fixed=TRUE)
sentence<- strsplit(sentence," ",fixed=TRUE)
(sentence)
sentence<- str_replace_all(articles[3],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
sentence<- strsplit(sentence,".",fixed=TRUE)
sentence
sentence<- strsplit(sentence," ",fixed=TRUE)
sentence<- strsplit(sentence[1]," ",fixed=TRUE)
sentence<- str_replace_all(articles[3],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
sentence<- strsplit(sentence,".",fixed=TRUE)
sentence<- strsplit(sentence[1]," ",fixed=TRUE)
sentence<- str_replace_all(articles[3],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
sentence<- strsplit(sentence," ",fixed=TRUE)
(sentence)
for (x in 1:length(articles)){
sentence<- str_replace_all(articles[x],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
sentence<- str_replace_all(sentence,pattern='.',replacement = "")
#SENTENCE<- strsplit(sentence,".",fixed=TRUE)
WORDS<- str_split(sentence," ")
df$word.count[x] <- nchar(WORDS)
}
for (x in 1:length(articles)){
sentence<- str_replace_all(articles[x],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
#sentence<- str_replace_all(sentence,pattern='.',replacement = "")
#SENTENCE<- strsplit(sentence,".",fixed=TRUE)
WORDS<- str_split(sentence," ")
df$word.count[x] <- nchar(WORDS)
}
View(df)
for (x in 1:length(articles)){
sentence<- str_replace_all(articles[x],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
SENTENCE<- strsplit(sentence,".",fixed=TRUE)
df$sentence.count[x] <- nchar(SENTENCE)
}
for (x in 1:length(articles)){
sentence<- str_replace_all(articles[x],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
SENTENCE<- strsplit(sentence,".",fixed=TRUE)
df$sentence.count[x] <- length(SENTENCE)
}
sentence<- str_replace_all(articles[3],pattern='\"',replacement = "")
sentence<- str_replace_all(sentence,pattern='\n',replacement = "")
SENTENCE<- strsplit(sentence,".",fixed=TRUE)
SENTENCE
typeof(SENTENCE)
SENTENCE[1]
SENTENCE[1][1]
SENTENCE[[2]]
SENTENCE
typeof(SENTENCE)
SENTENCE[[1]]
SENTENCE[[2]]
sentence
SENTENCE[[1]][1]
library(tinytex)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(png)
library(arules)
library(arulesViz)
install.packages("arules")
install.packages("arulesViz")
library(tinytex)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(png)
library(arules)
library(arulesViz)
# read transactions
groceries <- read.transactions("groceries.txt", format = "basket", sep = ",")
groceries_df <- as(groceries, "data.frame")
# see dimensions
dim(groceries)
# see basic summary
summary(groceries)
# see the first 10 transactions
arules::inspect(groceries[1:10])
# see the number of items in first 20 transactions
size(groceries[1:20])
# see list of item labels
itemLabels(groceries)
# Which groups of items are most frequently purchased together? (highest support)
# apriori alogrithm (support 5%)
grocRules <- apriori(groceries, parameter = list(supp = 0.05, conf = .0001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$support, decreasing=TRUE), ]
# Which groups of items have the highest lift?
# apriori alogrithm
grocRules <- apriori(groceries, parameter = list(supp = 0.001, conf = .001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$lift, decreasing=TRUE)[0:20], ]
# Interesting rules - which ones have higher confidence and higher lift
# apriori alogrithm
grocRulesInt <- apriori(groceries, parameter = list(supp = 0.001, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt,subset = lift > 2,by = 'support',decreasing = T)))
# Interesting rules - without whole milk and other vegetables?
# apriori alogrithm
grocRulesInt2 <- apriori(groceries, parameter = list(supp = 0.0015, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt2,subset=!(rhs %in% c('whole milk','other vegetables')) & lift > 3,by = 'support',decreasing = T)))
##########################################################
plot(grocRulesInt)
plot(grocRulesInt2)
plot(grocRules, method='two-key plot')
plot(grocRulesInt2)
plot(grocRulesInt)
plot(grocRulesInt2, measure = c("support", "lift"), shading = "confidence")
library(tinytex)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(png)
library(arules)
library(arulesViz)
# read transactions
groceries <- read.transactions("groceries.txt", format = "basket", sep = ",")
groceries_df <- as(groceries, "data.frame")
# see dimensions
dim(groceries)
# see basic summary
summary(groceries)
# see the first 10 transactions
arules::inspect(groceries[1:10])
# see the number of items in first 20 transactions
size(groceries[1:20])
# see list of item labels
itemLabels(groceries)
# plot most frequently sold items
itemFrequencyGGPlot <- function(x, topN) {
x %>%
itemFrequency %>%
sort %>%
tail(topN) %>%
as.data.frame %>%
tibble::rownames_to_column() %>%
ggplot(aes(reorder(rowname, `.`),`.`)) +
theme_bw() +
geom_col(fill="#F05483") +
coord_flip() +
labs(y="Frequency", x ="Item", title = "Most Frequently Purchased Grocery Items") +
theme(plot.title = element_text(hjust = 0.5))
}
itemFrequencyGGPlot(groceries, topN=20)
# plot least purchased items
itemFrequencyGGPlot <- function(x, topN) {
x %>%
itemFrequency %>%
sort %>%
head(topN) %>%
as.data.frame %>%
tibble::rownames_to_column() %>%
ggplot(aes(reorder(rowname, `.`),`.`)) +
theme_bw() +
geom_col(fill="#FFAAC4") +
coord_flip() +
labs(y="Frequency", x ="Item", title = "Least Frequently Purchased Grocery Items") +
theme(plot.title = element_text(hjust = 0.5))
}
itemFrequencyGGPlot(groceries, topN=20)
##########################################################
# APRIORI
##########################################################
# Which groups of items are most frequently purchased together? (highest support)
# apriori alogrithm (support 5%)
grocRules <- apriori(groceries, parameter = list(supp = 0.05, conf = .0001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$support, decreasing=TRUE), ]
# Which groups of items have the highest lift?
# apriori alogrithm
grocRules <- apriori(groceries, parameter = list(supp = 0.001, conf = .001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$lift, decreasing=TRUE)[0:20], ]
# Interesting rules - which ones have higher confidence and higher lift
# apriori alogrithm
grocRulesInt <- apriori(groceries, parameter = list(supp = 0.001, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt,subset = lift > 2,by = 'support',decreasing = T)))
# Interesting rules - without whole milk and other vegetables?
# apriori alogrithm
grocRulesInt2 <- apriori(groceries, parameter = list(supp = 0.0015, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt2,subset=!(rhs %in% c('whole milk','other vegetables')) & lift > 3,by = 'support',decreasing = T)))
plot(grocRules, method='two-key plot')
plot(grocRulesInt2, measure = c("support", "lift"), shading = "confidence")
knitr::include_graphics("gephiGrocery.pdf")
View(inc)
library(magrittr)
# get rid of scientific notation
options(scipen=999)
# read in data set
greenData <- read.csv("greenbuildings.csv")
# separate into green and non-green buildings
greenBuildings <- greenData %>% filter(green_rating == 1)
nonGreenBuildings <- greenData %>% filter(green_rating == 0)
subset_All <- greenData %>% filter(amenities==1&
cd_total_07>=966&
stories<=20&
stories>=10&
age<=10 | renovated==1)
subset_Green <- subset_All %>% filter(green_rating==1)
subset_Nongreen <- subset_All %>% filter(green_rating==0)
library(magrittr)
# get rid of scientific notation
options(scipen=999)
# read in data set
greenData <- read.csv("greenbuildings.csv")
# separate into green and non-green buildings
greenBuildings <- greenData %>% filter(green_rating == 1)
nonGreenBuildings <- greenData %>% filter(green_rating == 0)
# create subset of similar buildings based on what we know:
# mixed use building so look at buildings with amenities = 1
# new building so filter on buildings <10 yrs or renovated
# 15 stories so filter on buildings between 10-20 stories
# Austin, TX so filter on buildings with num cooling days > median of 966
subset_All <- greenData %>% filter(amenities==1&
cd_total_07>=966&
stories<=20&
stories>=10&
age<=10 | renovated==1)
subset_Green <- subset_All %>% filter(green_rating==1)
subset_Nongreen <- subset_All %>% filter(green_rating==0)
setwd("~/Documents/UT MSBA 2020-21")
setwd("~/Documents/UT MSBA 2020-21/Summer /PredictiveModeling/Part2_Exercices/STA380Exercises")
library(dplyr)
install.packages('tinytex')
install.packages("tinytex")
library(mosaic)
library(dplyr)
library(readr)
airport <- read.csv("ABIA.csv",header=T, na.strings=c("",NA))
airlinecodes <- read.csv("airline_codes.csv")
abia <- airport %>% left_join(airlinecodes,by="UniqueCarrier")
d3 = abia  %>% filter(DepDelay >=30) %>%
group_by(CRSDepTime) %>%
summarize(Frequency=n())
qplot(d3$CRSDepTime,
geom="histogram",
binwidth = 50,
main = "Delay Frequency Based on Time of Day",
xlab = "Time of Day (hhmm)",
ylab = "# of Delays",
fill=I('blue'),
col=I("red"),
alpha=I(.5))
qplot(d3$CRSDepTime,
geom="histogram",
binwidth = 50,
main = "Delay Frequency Based on Time of Day",
xlab = "Time of Day (hhmm)",
ylab = "# of Delays",
fill=I('blue'),
col=I("black"),
alpha=I(.5))
d4 = abia  %>% filter(DepDelay >=30) %>%
group_by(CRSDepTime) %>%
summarize(DepDelay)
d4.glm <- glm(cbind(DepDelay, 875 - DepDelay) ~ CRSDepTime,
data=d4, family=binomial(logit))
plot(DepDelay ~ CRSDepTime, data=d4, main='Depature Delay Length Based on Time of Day', xlab='Time of Day (hhmm0', ylab='Departure Delay Time')
