grocRulesDF[order(grocRulesDF$support, decreasing=TRUE), ]
# Which groups of items have the highest lift?
# apriori alogrithm
grocRules <- apriori(groceries, parameter = list(supp = 0.001, conf = .001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$lift, decreasing=TRUE)[0:20], ]
# Interesting rules - which ones have higher confidence and higher lift
# apriori alogrithm
grocRulesInt <- apriori(groceries, parameter = list(supp = 0.001, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt,subset = lift > 2,by = 'support',decreasing = T)))
# Interesting rules - without whole milk and other vegetables?
# apriori alogrithm
grocRulesInt2 <- apriori(groceries, parameter = list(supp = 0.0015, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt2,subset=!(rhs %in% c('whole milk','other vegetables')) & lift > 3,by = 'support',decreasing = T)))
##########################################################
plot(grocRulesInt)
plot(grocRulesInt2)
plot(grocRules, method='two-key plot')
plot(grocRulesInt2)
plot(grocRulesInt)
plot(grocRulesInt2, measure = c("support", "lift"), shading = "confidence")
library(tinytex)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(png)
library(arules)
library(arulesViz)
# read transactions
groceries <- read.transactions("groceries.txt", format = "basket", sep = ",")
groceries_df <- as(groceries, "data.frame")
# see dimensions
dim(groceries)
# see basic summary
summary(groceries)
# see the first 10 transactions
arules::inspect(groceries[1:10])
# see the number of items in first 20 transactions
size(groceries[1:20])
# see list of item labels
itemLabels(groceries)
# plot most frequently sold items
itemFrequencyGGPlot <- function(x, topN) {
x %>%
itemFrequency %>%
sort %>%
tail(topN) %>%
as.data.frame %>%
tibble::rownames_to_column() %>%
ggplot(aes(reorder(rowname, `.`),`.`)) +
theme_bw() +
geom_col(fill="#F05483") +
coord_flip() +
labs(y="Frequency", x ="Item", title = "Most Frequently Purchased Grocery Items") +
theme(plot.title = element_text(hjust = 0.5))
}
itemFrequencyGGPlot(groceries, topN=20)
# plot least purchased items
itemFrequencyGGPlot <- function(x, topN) {
x %>%
itemFrequency %>%
sort %>%
head(topN) %>%
as.data.frame %>%
tibble::rownames_to_column() %>%
ggplot(aes(reorder(rowname, `.`),`.`)) +
theme_bw() +
geom_col(fill="#FFAAC4") +
coord_flip() +
labs(y="Frequency", x ="Item", title = "Least Frequently Purchased Grocery Items") +
theme(plot.title = element_text(hjust = 0.5))
}
itemFrequencyGGPlot(groceries, topN=20)
##########################################################
# APRIORI
##########################################################
# Which groups of items are most frequently purchased together? (highest support)
# apriori alogrithm (support 5%)
grocRules <- apriori(groceries, parameter = list(supp = 0.05, conf = .0001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$support, decreasing=TRUE), ]
# Which groups of items have the highest lift?
# apriori alogrithm
grocRules <- apriori(groceries, parameter = list(supp = 0.001, conf = .001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame")
grocRulesDF[order(grocRulesDF$lift, decreasing=TRUE)[0:20], ]
# Interesting rules - which ones have higher confidence and higher lift
# apriori alogrithm
grocRulesInt <- apriori(groceries, parameter = list(supp = 0.001, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt,subset = lift > 2,by = 'support',decreasing = T)))
# Interesting rules - without whole milk and other vegetables?
# apriori alogrithm
grocRulesInt2 <- apriori(groceries, parameter = list(supp = 0.0015, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt2,subset=!(rhs %in% c('whole milk','other vegetables')) & lift > 3,by = 'support',decreasing = T)))
plot(grocRules, method='two-key plot')
plot(grocRulesInt2, measure = c("support", "lift"), shading = "confidence")
knitr::include_graphics("gephiGrocery.pdf")
View(inc)
library(magrittr)
# get rid of scientific notation
options(scipen=999)
# read in data set
greenData <- read.csv("greenbuildings.csv")
# separate into green and non-green buildings
greenBuildings <- greenData %>% filter(green_rating == 1)
nonGreenBuildings <- greenData %>% filter(green_rating == 0)
subset_All <- greenData %>% filter(amenities==1&
cd_total_07>=966&
stories<=20&
stories>=10&
age<=10 | renovated==1)
subset_Green <- subset_All %>% filter(green_rating==1)
subset_Nongreen <- subset_All %>% filter(green_rating==0)
library(magrittr)
# get rid of scientific notation
options(scipen=999)
# read in data set
greenData <- read.csv("greenbuildings.csv")
# separate into green and non-green buildings
greenBuildings <- greenData %>% filter(green_rating == 1)
nonGreenBuildings <- greenData %>% filter(green_rating == 0)
# create subset of similar buildings based on what we know:
# mixed use building so look at buildings with amenities = 1
# new building so filter on buildings <10 yrs or renovated
# 15 stories so filter on buildings between 10-20 stories
# Austin, TX so filter on buildings with num cooling days > median of 966
subset_All <- greenData %>% filter(amenities==1&
cd_total_07>=966&
stories<=20&
stories>=10&
age<=10 | renovated==1)
subset_Green <- subset_All %>% filter(green_rating==1)
subset_Nongreen <- subset_All %>% filter(green_rating==0)
setwd("~/Documents/UT MSBA 2020-21")
setwd("~/Documents/UT MSBA 2020-21/Summer /PredictiveModeling/Part2_Exercices/STA380Exercises")
library(dplyr)
install.packages('tinytex')
install.packages("tinytex")
library(mosaic)
library(dplyr)
library(readr)
airport <- read.csv("ABIA.csv",header=T, na.strings=c("",NA))
airlinecodes <- read.csv("airline_codes.csv")
abia <- airport %>% left_join(airlinecodes,by="UniqueCarrier")
d3 = abia  %>% filter(DepDelay >=30) %>%
group_by(CRSDepTime) %>%
summarize(Frequency=n())
qplot(d3$CRSDepTime,
geom="histogram",
binwidth = 50,
main = "Delay Frequency Based on Time of Day",
xlab = "Time of Day (hhmm)",
ylab = "# of Delays",
fill=I('blue'),
col=I("red"),
alpha=I(.5))
qplot(d3$CRSDepTime,
geom="histogram",
binwidth = 50,
main = "Delay Frequency Based on Time of Day",
xlab = "Time of Day (hhmm)",
ylab = "# of Delays",
fill=I('blue'),
col=I("black"),
alpha=I(.5))
d4 = abia  %>% filter(DepDelay >=30) %>%
group_by(CRSDepTime) %>%
summarize(DepDelay)
d4.glm <- glm(cbind(DepDelay, 875 - DepDelay) ~ CRSDepTime,
data=d4, family=binomial(logit))
plot(DepDelay ~ CRSDepTime, data=d4, main='Depature Delay Length Based on Time of Day', xlab='Time of Day (hhmm0', ylab='Departure Delay Time')
############ Exploratory Analysis ############
#### Graphs ####
charts.PerformanceSummary(returns,main='Portfolio Assets Summary', rf=.0071)
############ Setup ############
#### Libraries ####
library(quantmod)
library(timeSeries)
library(fPortfolio)
library(PerformanceAnalytics)
library(ggplot2)
# Bootstrapping
library(mosaic)
# Optimization
library(PortfolioAnalytics)
library(DEoptim)
library(ROI)
require(ROI.plugin.glpk)
require(ROI.plugin.quadprog)
#### Get Returns ####
# Portfolio
tickers <- c("AGG", "GLD", "VUG")
# Get AdjClose prices
adjClosePrices <- NULL
for (ticker in tickers){
adjClosePrices <- cbind(adjClosePrices,
quantmod::getSymbols(ticker, from="2015-08-01", to = '2020-08-01', verbose=FALSE, auto.assign=FALSE)[,6])
}
# keep only the dates that have closing prices for all tickers
adjClosePrices <- adjClosePrices[apply(adjClosePrices,1,function(x) all(!is.na(x))),]
# Convert AdjClose to Returns
returns <- Return.calculate(adjClosePrices)[-1,1:length(tickers)]
port1_weights <- c(0.8, 0.15, 0.05)
# Create portfoli (Rebalancing Daily)
port1_returns <- Return.portfolio(returns, weights = port1_weights, rebalance_on = "days")
#### Portfolio 2 ####
port2_weights <- c(0.1, 0.2, 0.7)
# Create portfoli (Rebalancing Daily)
port2_returns <- Return.portfolio(returns, weights = port2_weights, rebalance_on = "days")
#### Portfolio 3 ####
portfolioAssets <- colnames(returns)
init <- portfolio.spec(assets=portfolioAssets)
# Add Constraint (Weights sum to 1, ie full investment)
init <- add.constraint(portfolio=init,
type="weight_sum",
min_sum=1,
max_sum=1)
# Add Constraint (No shorting or borrowing, and no over/under investment)
init <- add.constraint(portfolio=init,
type="box",
min=0.05,
max=0.5
)
# Add Constraint (Target Return = 10% annual, 0.00055% daily)
init <- add.constraint(portfolio=init, type="return", return_target=0.00055)
# Add Objective (Minimize risk)
minvar <- add.objective(portfolio=init,
type='risk',
name='var'
)
# Optimize Portfolio
opt_maxret <- optimize.portfolio(R=returns, portfolio=minvar,
optimize_method="ROI",
trace=FALSE
)
# Create portfoli (Rebalancing Daily)
port3_returns <- Return.portfolio(returns, weights = opt_maxret$weights, rebalance_on = "days")
#### Bench Mark Portfolio ####
tickers <- c("SPY")
# Get AdjClose prices
adjClosePrices <- NULL
for (ticker in tickers){
adjClosePrices <- cbind(adjClosePrices,
quantmod::getSymbols(ticker, from="2015-08-01", to = '2020-08-01', verbose=FALSE, auto.assign=FALSE)[,6])
}
# keep only the dates that have closing prices for all tickers
adjClosePrices <- adjClosePrices[apply(adjClosePrices,1,function(x) all(!is.na(x))),]
# Convert AdjClose to Returns
#returns <- as.timeSeries((tail(adjClosePrices,-1) / as.numeric(head(adjClosePrices,-1)))-1)
market_returns <- Return.calculate(adjClosePrices)[-1,]
############ Exploratory Analysis ############
#### Graphs ####
charts.PerformanceSummary(returns,main='Portfolio Assets Summary', rf=.0071)
chart.RiskReturnScatter(returns, add.sharpe = TRUE, Rf=.0071)
chart.RelativePerformance(apply.weekly(returns, Return.cumulative), apply.weekly(market_returns, Return.cumulative), legend.loc='topright')
############ VaR Calculation ############
#### Conceptual ####
conceptualCaR <- function(portfolio, confidence){
portfolio_resampled <- mosaic::resample(portfolio)
portfolioReturns <- coredata(portfolio_resampled$portfolio.returns)
sortedPortfolioReturns <- sort(portfolioReturns, decreasing=FALSE)
port_dailyVar <- quantile(sortedPortfolioReturns,c(confidence))
port_monthlyVaR <- port_dailyVar*sqrt(20)
return(port_monthlyVaR)
}
# Analysis (Port 1)
# Bootstrap
monthlyVaR_bootstrap = do(1000)*conceptualCaR(port1_returns,.05)
port1_montlyVaR <- mean(monthlyVaR_bootstrap$X5.)
# Analysis (Port 2)
# Bootstrap
monthlyVaR_bootstrap = do(1000)*conceptualCaR(port2_returns,.05)
port2_montlyVaR <- mean(monthlyVaR_bootstrap$X5.)
# Analysis (Port 3)
# Bootstrap
monthlyVaR_bootstrap = do(1000)*conceptualCaR(port3_returns,.05)
port3_montlyVaR <- mean(monthlyVaR_bootstrap$X5.)
#### Normal ####
# Analysis (Port 1)
# Bootstrap
dailyVaR_bootstrap = do(1000)*PerformanceAnalytics::VaR(mosaic::resample(port1_returns))
port1_montlyVaR <- mean(dailyVaR_bootstrap$VaR)*sqrt(20)
# Analysis (Port 2)
# Bootstrap
dailyVaR_bootstrap = do(1000)*PerformanceAnalytics::VaR(mosaic::resample(port2_returns))
port2_montlyVaR <- mean(dailyVaR_bootstrap$VaR)*sqrt(20)
# Analysis (Port 3)
# Bootstrap
dailyVaR_bootstrap = do(1000)*PerformanceAnalytics::VaR(mosaic::resample(port3_returns))
port3_montlyVaR <- mean(dailyVaR_bootstrap$VaR)*sqrt(20)
#### Graphing ####
# Returns
port1_monthly = apply.weekly(port1_returns, Return.cumulative)
port1 <- data.frame(port1_monthly)
port1$portfolio <- 'Portfolio 1'
port2_monthly = apply.weekly(port2_returns, Return.cumulative)
port2 <- data.frame(port2_monthly)
port2$portfolio <- 'Portfolio 2'
port3_monthly = apply.weekly(port3_returns, Return.cumulative)
port3 <- data.frame(port3_monthly)
port3$portfolio <- 'Portfolio 3'
portfolioReturns <- rbind(port1, port2, port3)
# Portfolio VaRs
port1_VaR <- data.frame(PortfolioVaR=port1_montlyVaR)
port1_VaR$portfolio <- 'Portfolio 1'
port2_VaR <- data.frame(PortfolioVaR=port2_montlyVaR)
port2_VaR$portfolio <- 'Portfolio 2'
port3_VaR <- data.frame(PortfolioVaR=port3_montlyVaR)
port3_VaR$portfolio <- 'Portfolio 3'
portfolioVaRs <- rbind(port1_VaR, port2_VaR, port3_VaR)
ggplot(portfolioReturns, aes(portfolio.returns, fill = portfolio)) +
geom_density(alpha = 0.2) +
geom_vline(data=portfolioVaRs, aes(xintercept=PortfolioVaR, color = portfolio), linetype="dashed") +
ggtitle("Historical Returns (Monthly) vs VaR") +
xlab("Returns")
library(dplyr)
library(klaR)
social_marketing <- read.csv('social_marketing.csv')
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X
## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
x / total_col
})
pcts$X = social_marketing_filtered$X
pcts = as.data.frame(pcts)
#k-means cluster
X = pcts[ , !(names(pcts) %in% c('X'))]
X = scale(X, center=TRUE, scale=TRUE)
clusters <- kmeans(X, 6, nstart=25)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
cluster1 <- clusters$center[1,]*sigma + mu
cluster2 <- clusters$center[2,]*sigma + mu
cluster3 <- clusters$center[3,]*sigma + mu
cluster4 <- clusters$center[4,]*sigma + mu
cluster5 <- clusters$center[5,]*sigma + mu
cluster6 <- clusters$center[6,]*sigma + mu
#sort each cluster's values
cluster1 <- as.data.frame(cluster1)
cluster1$categories <- row.names(cluster1)
cluster1[order(cluster1$cluster1, decreasing = TRUE),]
cluster2 <- as.data.frame(cluster2)
cluster2$categories <- row.names(cluster2)
cluster2[order(cluster2$cluster2, decreasing = TRUE),]
cluster3 <- as.data.frame(cluster3)
cluster3$categories <- row.names(cluster3)
cluster3[order(cluster3$cluster3, decreasing = TRUE),]
cluster4 <- as.data.frame(cluster4)
cluster4$categories <- row.names(cluster4)
cluster4[order(cluster4$cluster4, decreasing = TRUE),]
cluster5 <- as.data.frame(cluster5)
cluster5$categories <- row.names(cluster5)
cluster5[order(cluster5$cluster5, decreasing = TRUE),]
cluster6 <- as.data.frame(cluster6)
cluster6$categories <- row.names(cluster6)
cluster6[order(cluster6$cluster6, decreasing = TRUE),]
library(dplyr)
library(factoextra)
install.packages("factoextra")
install.packages("ggfortify")
library(dplyr)
library(factoextra)
library(ggfortify)
library(ggplot2)
social_marketing <- read.csv('social_marketing.csv')
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X
## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
x / total_col
})
pcts$X = social_marketing_filtered$X
pcts = as.data.frame(pcts)
#rank percentages
#pcts_rank = lapply(pcts, rank)
#k-means cluster
X = pcts[ , !(names(pcts) %in% c('X'))]
res.pca = prcomp(X, scale = FALSE)
fviz_eig(res.pca, addlabels = TRUE)
# Eigenvalues
eig.val <- get_eigenvalue(res.pca)
#eig.val
# Results for Variables
res.var <- get_pca_var(res.pca)
#res.var$coord # Coordinates
#res.var$contrib # Contributions to the PCs
#res.var$cos2 # Quality of representation
# Results for individuals
#res.ind <- get_pca_ind(res.pca)
#res.ind$coord # Coordinates
#res.ind$contrib # Contributions to the PCs
#res.ind$cos2 # Quality of representation
#autoplot(res.pca)
#princomp(X, cor = FALSE, scores = TRUE)
#X = scale(X, center=TRUE, scale=TRUE)
fviz_pca_var(res.pca, col.var = "black")
library("corrplot")
corrplot(res.var$cos2[,1:10], is.corr=FALSE)
#res.var$contrib # Contributions to the PCs
#res.var$cos2 # Quality of representation
# Results for individuals
#res.ind <- get_pca_ind(res.pca)
#res.ind$coord # Coordinates
#res.ind$contrib # Contributions to the PCs
#res.ind$cos2 # Quality of representation
#autoplot(res.pca)
#princomp(X, cor = FALSE, scores = TRUE)
#X = scale(X, center=TRUE, scale=TRUE)
fviz_pca_var(res.pca, col.var = "black")
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X
## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
x / total_col
})
pcts$X = social_marketing_filtered$X
pcts = as.data.frame(pcts)
#rank percentages
#pcts_rank = lapply(pcts, rank)
#k-means cluster
X = pcts[ , !(names(pcts) %in% c('X'))]
res.pca = prcomp(X, scale = FALSE)
fviz_eig(res.pca, addlabels = TRUE)
library(dplyr)
library(factoextra)
library(ggfortify)
library(ggplot2)
social_marketing <- read.csv('social_marketing.csv')
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X
## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
x / total_col
})
pcts$X = social_marketing_filtered$X
pcts = as.data.frame(pcts)
#rank percentages
#pcts_rank = lapply(pcts, rank)
#k-means cluster
X = pcts[ , !(names(pcts) %in% c('X'))]
res.pca = prcomp(X, scale = FALSE)
fviz_eig(res.pca, addlabels = TRUE)
library("corrplot")
corrplot(res.var$cos2[,1:10], is.corr=FALSE)
View(social_marketing_filtered)
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
library(dplyr)
library(factoextra)
library(ggfortify)
library(ggplot2)
social_marketing <- read.csv('social_marketing.csv')
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X
## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
x / total_col
})
pcts$X = social_marketing_filtered$X
pcts = as.data.frame(pcts)
#rank percentages
#pcts_rank = lapply(pcts, rank)
#k-means cluster
X = pcts[ , !(names(pcts) %in% c('X'))]
res.pca = prcomp(X, scale = FALSE)
fviz_eig(res.pca, addlabels = TRUE)
social_marketing <- read.csv('social_marketing.csv')
social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X
## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
x / total_col
})
pcts$X = social_marketing_filtered$X
pcts = as.data.frame(pcts)
#k-means cluster
X = pcts[ , !(names(pcts) %in% c('X'))]
X = scale(X, center=TRUE, scale=TRUE)
clusters <- kmeans(X, 6, nstart=25)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")
cluster1 <- clusters$center[1,]*sigma + mu
cluster2 <- clusters$center[2,]*sigma + mu
cluster3 <- clusters$center[3,]*sigma + mu
cluster4 <- clusters$center[4,]*sigma + mu
cluster5 <- clusters$center[5,]*sigma + mu
cluster6 <- clusters$center[6,]*sigma + mu
#sort each cluster's values
cluster1 <- as.data.frame(cluster1)
cluster1$categories <- row.names(cluster1)
cluster1[order(cluster1$cluster1, decreasing = TRUE),]
cluster2 <- as.data.frame(cluster2)
cluster2$categories <- row.names(cluster2)
cluster2[order(cluster2$cluster2, decreasing = TRUE),]
cluster3 <- as.data.frame(cluster3)
cluster3$categories <- row.names(cluster3)
cluster3[order(cluster3$cluster3, decreasing = TRUE),]
cluster4 <- as.data.frame(cluster4)
cluster4$categories <- row.names(cluster4)
cluster4[order(cluster4$cluster4, decreasing = TRUE),]
cluster5 <- as.data.frame(cluster5)
cluster5$categories <- row.names(cluster5)
cluster5[order(cluster5$cluster5, decreasing = TRUE),]
cluster6 <- as.data.frame(cluster6)
cluster6$categories <- row.names(cluster6)
cluster6[order(cluster6$cluster6, decreasing = TRUE),]
