---
title: "Predictive Modeling Exercises"
author: "Kessiena Ofunrein, Jordan Pflum, Jennifer Robinson, and Katelyn Vincent"
date: "8/8/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
editor_options:
  chunk_output_type: console
header-includes:
- \usepackage{placeins}
- \usepackage{tikz}
- \usetikzlibrary{mindmap,shadows}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=5.5, fig.height=3)
```
# Question 1 
```{r,include=FALSE}
library(dplyr)
library(magrittr)
# get rid of scientific notation
options(scipen=999)

# read in data set
greenData <- read.csv("greenbuildings.csv")

# separate into green and non-green buildings
greenBuildings <- greenData %>% filter(green_rating == 1)
nonGreenBuildings <- greenData %>% filter(green_rating == 0)

# create subset of similar buildings based on what we know:
# mixed use building so look at buildings with amenities = 1
# new building so filter on buildings <10 yrs or renovated
# 15 stories so filter on buildings between 10-20 stories
# Austin, TX so filter on buildings with num cooling days > median of 966

subset_All <- greenData %>% filter(amenities==1&
                                   cd_total_07>=966&
                                   stories<=20&
                                   stories>=10&
                                   age<=10 | renovated==1)

subset_Green <- subset_All %>% filter(green_rating==1)
subset_Nongreen <- subset_All %>% filter(green_rating==0)
```
Though potentially accurate in their findings, the conclusions of the initial analysis regarding the economic viability of “going green” were based on flawed logic.  The employee filtered out buildings with occupancy rates of less than ten percent, assuming that the occupancy rate for the new building will be over ten percent without providing evidence to support his theory.  To strengthen his argument, he could have used a visual like the one below, which shows the leasing rates by size for all of the buildings in the data set.  Based on this, it appears that there are in fact no ‘green’ buildings in the data set with leasing rates (aka occupancy rates) lower than 10 percent. 
```{r,fig.align='center',echo=FALSE}
#####################################################
# plot leasing rate vs size, all data
#####################################################
library(ggplot2)
# plot leasing rate vs size (all data)
ggplot(data = greenData) + 
  geom_point(aes(x = leasing_rate, y = size, color=factor(green_rating))) +
  labs(color = "Green Building") +
  # add colors and legend labels
  scale_colour_manual(values = c("#01148A","#0AD626"),labels = c("No","Yes")) +
  ggtitle("Leasing Rate vs. Size") +
  # center plot title
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = "Leasing Rate",y = "Size (Sq. ft)")
```
While the idea of filtering the dataset is a good one, the filtering approach did not take into account the information that is known about the new project - that it will be a 15-story mixed-use building in Austin, TX measuring 250,000 sq. ft.  This information, though limited, allows us to narrow our focus to buildings with similar properties.  For example, we created a subset of the data that contained information about buildings that: were between 10-20 stories, were either less than 10 years old or had been renovated, had amenities (since the building will be mixed-use), and had annual cooling degree days over 966 (the median for the whole dataset).  Since there were no variables directly related to location, we used high cooling degree days to filter on locations in similar climates to Austin, TX (if not similar geographic regions).  This subset contained 407 buildings, including 33 designated as ‘green buildings’.  
Subsetting the data yields different information about the rent of green vs non-green buildings.  Below is the rent distribution for all buildings in the dataset, which would seem to support the original analysis.

```{r,fig.align='center',echo=FALSE}
#####################################################
# distribution of rental prices, all buildings
#####################################################

plot(density(greenBuildings$Rent), main="Rent Distribution, All Buildings", ylim=c(0,.045), col = "#0AD626", xlab = "Rent Per Square Foot")
lines(density(nonGreenBuildings$Rent),col = "#01148A")
legend(100, .035, legend=c("Green", "Not Green"),
       col=c("#0AD626", "#01148A"), lty=1:2, cex=0.8)
```
However, when analyzing green buildings in the subset of data (containing information on buildings similar to the new one), it appears that the rent distribution skews slightly more towards less expensive rental prices.  

```{r,fig.align='center',echo=FALSE}
# green buildings compared (all to subset)
plot(density(greenBuildings$Rent), main="Rent Distribution, Green Buildings", ylim=c(0,.06), xlim=c(0,100), col = "#69E878", xlab = "Rent Per Square Foot")
lines(density(subset_Green$Rent),col = "#155B2F")
legend(70, .05, legend=c("All Data", "Subset"),
       col=c("#69E878", "#155B2F"), lty=1:2, cex=0.8)
```
The median rent for green buildings in the subset is 24.36 dollars vs 22.50 dollars, this is still higher, but both are significantly less than the rental prices in the original analysis.  
```{r,fig.align='center',echo=FALSE}
#####################################################
# plot dist. of rent prices for subset
#####################################################
green.denplot <- function(df,TITLE){
  attach(df)
  median.column = round(median(df$Rent),digits=2)
  graph = df %>%
    ggplot(aes(x=Rent))+
    geom_density(fill="#0AD626",alpha=0.5)+
    scale_x_log10()+
    geom_vline(xintercept = median.column,size=1,color="black")+
    geom_label(aes(x = median.column, y = 0.5, label = paste("Median Rent: $", median.column)), fill = "white")+
    labs(y="Density", x ="Rent Per Sq.Ft.", title = TITLE) +
    theme(plot.title = element_text(hjust = 0.5))
  return(graph) 
}

green.denplot(subset_Green, "Rent Distribution (Subset, Green Buildings)")

nongreen.denplot <- function(df,TITLE){
  attach(df)
  median.column = round(median(df$Rent),digits=2)
  graph = df %>%
    ggplot(aes(x=Rent))+
    geom_density(fill="#01148A",alpha=0.5)+
    scale_x_log10()+
    geom_vline(xintercept = median.column,size=1,color="black")+
    geom_label(aes(x = median.column, y = 0.5, label = paste("Median Rent: $", median.column)), fill = "white")+
    labs(y="Density", x ="Rent Per Sq.Ft.", title = TITLE) +
    theme(plot.title = element_text(hjust = 0.5))
  return(graph) 
}

nongreen.denplot(subset_Nongreen, "Rent Distribution (Subset, Non-Green Buildings)")
```
Additionally, while the original analysis factored in expected revenue differences, it failed to account for differences in electricity cost savings over the year from “going green”.  Only three percent of all buildings (and six percent of green buildings) in the original dataset have rent quoted on a “net contract” basis, where the tenant pays for utility costs, which means that the majority of utility costs are paid by the building owners.  
Estimated annual electricity costs for a non-green building were computed using the building size (250,000 sq ft), avg number of kWh per square foot for commercial buildings (22.5), and median electricity costs for buildings in the subset.  The annual electricity costs for the green building assumed a 25 percent reduction in energy usage.  To compute revenue, the median occupancy rate was multiplied by the median rent (for green and non-green buildings in the subset, separately).  Below is the break-even analysis for a new green building, which takes into account the difference in costs and expected revenues compared to the construction of a non-green building.  Based on this analysis, the additional costs of the green building would be recouped in 9.5 years.
```{r,fig.align='center',echo=FALSE}
#####################################################
# plot break even analysis
#####################################################

# fixed costs
fixed_cost <- 5000000

# annual electricity costs
# average kwh for commercial buildings is 22.5
# https://www.iotacommunications.com/blog/benchmarking-commercial-building-energy-use-per-square-foot/#:~:text=According%20to%20the%20Department%20of,are%20consumed%20by%20refrigeration%20%26%20equipment.
annual_kwh_persqf <- 22.5
building_size_sqf <- 250000
elec_cost <- median(subset_All$Electricity_Costs)
ann_elec_usage_other <- annual_kwh_persqf*building_size_sqf
ann_elec_costs_other <- ann_elec_usage_other*elec_cost
# average 25% reduction in energy usage for green buildings
# https://www.iotacommunications.com/blog/green-building-benefits/
ann_elec_usage_green <- ann_elec_usage_other*0.75
ann_elec_costs_green <- ann_elec_usage_green*elec_cost
delta_elec_costs <- ann_elec_costs_green - ann_elec_costs_other

# annual revenue
occ_rate_other <- median(subset_Nongreen$leasing_rate)/100
rent_other <- median(subset_Nongreen$Rent)
ann_revenue_other <- building_size_sqf*occ_rate_other*rent_other
occ_rate_green <- median(subset_Green$leasing_rate)/100
rent_green <- median(subset_Green$Rent)
ann_revenue_green <- building_size_sqf*occ_rate_green*rent_green
delta_revenue <- ann_revenue_green - ann_revenue_other

years <- c(seq(0, 15))
fixed_cost <- c(rep(5000000, length(years)))
variable_cost <- delta_elec_costs*years
total_cost <- fixed_cost + variable_cost
revenue <- delta_revenue*years
contribution_margin <- delta_revenue - delta_elec_costs
break_even_years <- round(fixed_cost[1]/contribution_margin,digits=1)
breakeven <- data.frame(years, fixed_cost, variable_cost, total_cost, revenue)

# plot break-even analysis
ggplot(data = breakeven, aes(x = years)) +
  geom_line(aes(y = fixed_cost,
                col = "Fixed Cost")) +
  geom_line(aes(y = variable_cost,
                col = "Variable Cost")) +
  geom_line(aes(y = total_cost,
                col = "Total Cost")) +
  geom_line(aes(y = revenue,
                col = "Revenue")) +
  geom_segment(aes(x = break_even_years, xend = break_even_years, 
                   y = 0, yend = break_even_years*delta_revenue),
               linetype = "dashed") +
  geom_segment(aes(x = 0, xend = break_even_years, 
                   y = break_even_years*delta_revenue, yend = break_even_years*delta_revenue),
               linetype = "dashed") +
  geom_point(aes(x = break_even_years, 
                 y= break_even_years*delta_revenue), 
             colour = "black", size = 4) +
  annotate("text", x = break_even_years, y = 0, label = paste("Break Even Point:", break_even_years,"years")) +
  scale_color_manual(labels = c("Fixed Cost", "Delta Revenue", "Total Cost", "Delta Variable Cost"),
                     values = c("Fixed Cost" = "black", "Variable Cost" = "blue", "Total Cost" = "red", "Revenue" = "green")) +
  geom_ribbon(data = breakeven[breakeven$total_cost >= breakeven$revenue, ], aes(x = years, ymin = revenue, ymax = total_cost), fill = "red", alpha = 0.15) +
  geom_ribbon(data = breakeven[breakeven$total_cost <= breakeven$revenue, ], aes(x = years, ymin = total_cost, ymax = revenue), fill = "green", alpha = 0.15) +
  labs(title = "Break-Even Analysis (Green Building)",
       x = "Year",
       y = "Dollars",
       color = NULL) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(labels=scales::dollar_format())
```
One possible confounding variable for the relationship between rent and green status in the initial analysis could be the building class.  Almost 80% of all green buildings are Class A buildings, which are the highest quality buildings in a market, whereas only 36 percent of non-green buildings have a Class A designation.  Since Class A buildings command a much higher rent than other building types, what appeared to be a higher rent for ‘green’ buildings could be attributed to the fact that most of them are of a higher quality building type.  
```{r,fig.align='center',echo=FALSE}
#####################################################
# plot rent distribution for class a buildings (all)
#####################################################

classABuildings <- greenData %>% filter(class_a == 1)
otherClassBuildings <- greenData %>% filter(class_a == 0)

classa.denplot <- function(df,TITLE){
  attach(df)
  median.column = round(median(df$Rent),digits=2)
  graph = df %>%
    ggplot(aes(x=Rent))+
    geom_density(fill="#45CCE4",alpha=0.5)+
    scale_x_log10()+
    geom_vline(xintercept = median.column,size=1,color="black")+
    geom_label(aes(x = median.column, y = 0.5, label = paste("Median Rent: $", median.column)), fill = "white")+
    labs(y="Density", x ="Rent Per Sq.Ft.", title = TITLE) +
    theme(plot.title = element_text(hjust = 0.5))
  return(graph) 
}

classa.denplot(classABuildings, "Rent Distribution (All Class A Buildings)")
classa.denplot(otherClassBuildings, "Rent Distribution (All Non-Class A Buildings)")

```

# Question 2 
The figures below portray a story about the flights arriving in and departing from the Austin Bergstrom International Airport in 2008. We looked at traits of flights and airlines from a few different perspectives to convey an accurate image of what the airport and travelers experienced that year. This analysis of the data is extremely useful for a few different potential clients. Firstly, travelers are the sustenance of this industry and are impacted by the factors we are analyzing. By providing them with some of these graphs, travelers will be more informed about some things they are likely to experience temporally, geographically, or with a specific airline. Secondly, airport officials would benefit greatly from our analysis because it is advantageous to know what one is likely to experience with each airport. Airports are businesses with reputations and standards. Notoriety for delayed flights is very useful information for preparations and strategic planning. 
```{r,include=FALSE}
library(magrittr)
library(ggplot2)
library(ggmosaic)
library(varhandle)
library(lubridate)
library(dplyr)
#################################   DATE COLUMN & MORE ###########################
#set up the data frame and graphs
#FILES NEEDED IN FOLDER:
    #ABIA.csv
    #airline_codes.csv
###############################################################################3
# add column for airline
airport <- read.csv("ABIA.csv",header=T, na.strings=c("",NA))
airlinecodes <- read.csv("airline_codes.csv")
abia <- airport %>% left_join(airlinecodes,by="UniqueCarrier")

# replace n/a cells with zero
abia[is.na(abia)] <- 0
abia[abia=='NULL'] <- 0

# recode cancellation code
abia$CancellationCode <- recode(abia$CancellationCode, 
                                "A"="Carrier",
                                "B"="Weather",
                                "C"="NAS")

abia$CancellationCode <- factor(abia$CancellationCode, 
                                levels= c("Carrier",
                                          "Weather",
                                          "NAS"))

# recode days of week
abia$DayOfWeek <- recode(abia$DayOfWeek, 
                         "7"="Sunday",
                         "1"="Monday",
                         "2"="Tuesday",
                         "3"="Wednesday",
                         "4"="Thursday",
                         "5"="Friday",
                         "6"="Saturday")

abia$DayOfWeek <- factor(abia$DayOfWeek, 
                         levels= c("Monday", 
                                   "Tuesday",
                                   "Wednesday", 
                                   "Thursday",
                                   "Friday", 
                                   "Saturday", 
                                   "Sunday"))

# recode months
abia$MonthName <- recode(abia$Month, 
                         "1"="Jan",
                         "2"="Feb",
                         "3"="Mar",
                         "4"="April",
                         "5"="May",
                         "6"="June",
                         "7"="July",
                         "8"="Aug",
                         "9"="Sept",
                         "10"="Oct",
                         "11"="Nov",
                         "12"="Dec")

abia$MonthName <- factor(abia$MonthName, 
                         levels= c("Jan",
                                   "Feb",
                                   "Mar",
                                   "April",
                                   "May",
                                   "June",
                                   "July",
                                   "Aug",
                                   "Sept",
                                   "Oct",
                                   "Nov",
                                   "Dec"))

# create date column
abia$date <- as.Date(with(abia, paste(Year, Month, DayofMonth,sep="-")), "%Y-%m-%d")
abia$weeknum <- isoweek(abia$date)

# set colorblind-friendly palette
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
###############################################################################
#SUBSET                   most popular airlines (above mean)
commonair <- data.frame(count(abia, vars = Airline)) #df of airlines and frequency in abia data frame
airline.counts <- table(abia$Airline) #table of counts similar to commonair
barplot(airline.counts, main="Frequency of Airlines",
        xlab="Airline")

airline.mean <- mean(commonair$n) #mean frequency of airlines in abia
x.sub <- commonair %>%
  filter(n >= airline.mean) #data frame of list of airlines who's frequency is > median
airlines <- x.sub$vars #list of popular airlines

popular.abia <- subset(abia, Airline %in% airlines) #data frame with info for each flight for popular airlines
###############################################################################
#SUBSET                   FILTER OUT SHORT DELAYS AND MOST POPULAR AIRLINES AIRLINES~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``
summary(popular.abia$ArrDelay)
m<- mean(popular.abia$ArrDelay, na.rm=TRUE) #exclude NA from calculations 
longARRdelays <-popular.abia %>% filter(ArrDelay > m)

summary(abia$DepDelay)
n<- mean(popular.abia$DepDelay, na.rm=TRUE) #exclude NA from calculations 
longDEPdelays <-popular.abia %>% filter(DepDelay > n)
```
The line graph figures below are a time series analysis of how the length of flight delays changes throughout the year 2018. The graphs are relatively similar and have peaks during similar times. They are separated by arriving and departing flights, essentially whether the plane was late coming in or late coming out. It is to be expected that a change in one would impact an airport’s schedule thus resulting in changes in the other. For both graphs there are peaks in months like March, June, and December. It is easy to deduce the cause of that could be holidays or related to school schedules like Spring Break. For a more concrete picture further analysis should be conducted. The airlines graphed here have more flights than the average airline and the delays are considered long delays because they are in the 75th percentile. 
```{r,fig.align='center', echo=FALSE} 
##############################    LINE GRAPHS   ##############################

#LINE PLOT MONTH VS AVERAGE ARRIVAL DELAY TIME BY AIRLINE FOR POPULAR AIRLINES -----------------------------DONE 
AvgArrDelay <- longARRdelays %>%
  group_by(Month, Airline) %>%
  summarize(avgArrDelay = mean(ArrDelay, na.rm = TRUE))
ggplot(AvgArrDelay, aes(x=Month, y=avgArrDelay, group=Airline))+ 
  scale_fill_brewer(palette="Paired") +
  geom_line(aes(color=Airline))+
  geom_point(aes(color=Airline))+ 
  ggtitle("Average Arrival Delay Over Time (Year:2018)") +
  xlab("Airline") + 
  ylab("Average Arrival Delay")
#---------------------------------------------------------------------------------------END

#LINE PLOT MONTH VS AVERAGE DEPARTURE DELAY TIME BY AIRLINE FOR POPULAR AIRLINES -----------------------------DONE 
AvgDepDelay <- longARRdelays %>%
  group_by(Month, Airline) %>%
  summarize(avgDepDelay = mean(DepDelay, na.rm = TRUE))
ggplot(AvgDepDelay, aes(x=Month, y=avgDepDelay, group=Airline))+ 
  scale_fill_brewer(palette="Paired") +
  geom_line(aes(color=Airline))+
  geom_point(aes(color=Airline))+ 
  ggtitle("Average Departure Delay Over Time (Year:2018)") +
  xlab("Airline") + 
  ylab("Average Departure Delay")
#---------------------------------------------------------------------------------------END
```
As we transition our focus from how flights are delayed for longer periods of time seasonally, we can also look at the different types of cancellations and delays. The graph below enables us to compare three types of cancellation codes: Carrier, Weather, and NAS. This plot is clearly able to show that NAS delays do not account for many delays and truly do not compare to weather or carrier delays. 
```{r,fig.align='center', echo=FALSE}
# aggregate the data set by more than one factor
d2 = abia  %>% filter(abia$CancellationCode != 0) %>%
  group_by(CancellationCode, DayOfWeek) %>%
  summarize(numcancels=n()/52)

# plot
ggplot(data = d2) + 
  geom_bar(mapping = aes(x=DayOfWeek, y=numcancels, fill=CancellationCode),
           position="dodge", stat='identity') +
  labs(x = "Weekday", fill = "Cancellation Code") +
  scale_fill_manual(values=cbPalette) +
  theme_bw() +
  ggtitle("Average Number of Cancelled Flights by Weekday in 2008") +
  # center plot title
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = "Weekday",y = "Avg # Cancelled Flights")
```
In the graph below, we look at a similar trend but over months so we can look at a seasonal picture of the flights. Some interesting anomalies are found in April and September. Based on knowledge of seasonal changes in weather, one could conclude that the significant flux in weather delays in September could be explained by hurricane season. As for April, carrier delays could increased significantly because American Airlines canceled a lot of flights due to not meeting government regulation standards. 
```{r,fig.align='center',echo=FALSE}
# aggregate the data set by more than one factor
d12 = abia %>% filter(abia$CancellationCode != 0) %>%
  group_by(CancellationCode, MonthName) %>%
  summarize(numcancelled=n())

d13 = abia %>%
  group_by(MonthName) %>%
  summarize(numflights=n(),.groups = 'drop') %>%
  ungroup() %>%
  arrange(desc(numflights))

combo4 <-merge(x=d12,y=d13,by="MonthName",all.x=TRUE)
combo4$cancperc <- combo4$numcancelled/combo4$numflights
combo4$numcancelled <- NULL
combo4$numflights <- NULL

# plot
ggplot(data = combo4) + 
  geom_bar(mapping = aes(x=MonthName, y=cancperc, fill=CancellationCode),
           position="dodge", stat='identity') +
  labs(x = "Month", fill = "Cancellation Code") +
  scale_fill_manual(values=cbPalette) +
  theme_bw() +
  ggtitle("Percent of Flights Cancelled by Month in 2008") +
  # center plot title
  theme(plot.title = element_text(hjust = 0.5))+
  # add labels
  labs(x = "2008",y = "% Flights Cancelled")

```
Strategic planning is a fundamental component of how airports sustain themselves. Knowing that an airport or region is notorious for delays or that they are notorious for this seasonally, plans can be made to accommodate to yield the best options for travelers. A simple way to summarize this information in a plot is through geographical mapping. Figure ___ shows this; airports with larger volumes of flights arriving in or departing from Austin have larger and brighter circles. An airport official should show more concern for a large dot that is lightly shaded, as this airport has a lot of flights outgoing to the Austin airport so schedules in these airports are integral to an efficient airport.
```{r,include=FALSE}
library(splitstackshape)
library(ggmap)
# Load Data
abia <- read.csv('ABIA.csv')
airportCodes <- read.csv('airportCodes.csv')

# Rename Airport Codes iata_code and get unique codes
abiaDestination <- abia %>% select(Dest) %>% rename('iata_code' = Dest) %>% distinct()
abiaOrgin <- abia %>% select(Origin) %>% rename('iata_code' = Origin) %>% distinct()

# Get destination codes
abiaDestination <- merge(abiaDestination, airportCodes, by='iata_code') %>% 
  distinct(iata_code, .keep_all= TRUE) %>% 
  select(iata_code, coordinates) 

# Get Origin Codes
abiaOrgin <- merge(abiaOrgin, airportCodes, by='iata_code') %>% 
  distinct(iata_code, .keep_all= TRUE) %>% 
  select(iata_code, coordinates)

# Split into Longitude and Latitude (Destination)
abiaDestination <- cSplit(abiaDestination, 'coordinates', sep=", ", type.convert=FALSE) %>% 
  rename('latitude' = coordinates_1,
         'longitude' = coordinates_2
  )
abiaDestination$latitude <- as.numeric(abiaDestination$latitude)
abiaDestination$longitude <- as.numeric(abiaDestination$longitude)

# Split into Longitude and Latitude (Orgin)
abiaOrgin <- cSplit(abiaOrgin, 'coordinates', sep=", ", type.convert=FALSE) %>% 
  rename('latitude' = coordinates_1,
         'longitude' = coordinates_2
  )
abiaOrgin$latitude <- as.numeric(abiaOrgin$latitude)
abiaOrgin$longitude <- as.numeric(abiaOrgin$longitude)


# Merge Dataframe back to original dataframe (Desitnation)
abiaDestination <- abiaDestination %>% rename('Dest' = iata_code)
abia <- merge(abia, abiaDestination, by='Dest') %>% rename('Dest_Longitude' = longitude,
                                                                       'Dest_Latitude' = latitude)

# Merge Dataframe back to original dataframe (Orgina)
abiaOrgin <- abiaOrgin %>% rename('Origin' = iata_code)
abia <- merge(abia, abiaOrgin, by='Origin') %>% rename('Origin_Longitude' = longitude,
                                                                   'Origin_Latitude' = latitude)
# Get Density of Departures
densityOfDepatrues <- abia %>% count(Dest)
abia <- merge(abia, densityOfDepatrues, by='Dest') %>% rename('Density_Destination' = n)

# Get Density of Arrivals
densityOfArrivals <- abia %>% count(Origin)
abia <- merge(abia, densityOfArrivals, by='Origin') %>% rename('Density_Arrivals' = n)
```

```{r,fig.align='center',include=FALSE}
# Mapping
register_google(key = "AIzaSyDjF7I4JjprxlUM4ZFB_0AScyEME5HsI-s")
```
```{r,fig.align='center',echo=FALSE}
##############################    GEO PLOT   ##############################
#######################    DEPARTURES PER AIRPORT   ########################

usa <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(bbox = usa, zoom = 4, maptype = "toner-lite")


# Number of Depatures per Airport
numDepatures <- abia %>% filter(Dest != 'AUS') %>% 
  select(Dest, Dest_Longitude, Dest_Latitude, Density_Destination) %>% 
  group_by(Dest) %>% 
  summarize('longitude' = mean(Dest_Longitude, na.rm = TRUE),
            'latitude' = mean(Dest_Latitude, na.rm = TRUE),
            'Number_Flight_Depatures' = mean(Density_Destination, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=numDepatures, aes(x=longitude, y=latitude, color=Number_Flight_Depatures, size=Number_Flight_Depatures))

# Number of Arrivals per Airport
numArrivals <- abia %>% filter(Origin != 'AUS') %>% 
  select(Origin, Origin_Longitude, Origin_Latitude, Density_Arrivals) %>% 
  group_by(Origin) %>% 
  summarize('longitude' = mean(Origin_Longitude, na.rm = TRUE),
            'latitude' = mean(Origin_Latitude, na.rm = TRUE),
            'Number_Flight_Arrivals' = mean(Density_Arrivals, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=numArrivals, aes(x=longitude, y=latitude, color=Number_Flight_Arrivals, size=Number_Flight_Arrivals))

```
Now that we have an understanding of where the most frequented airports are, we can now look at a similar map that tells us how timely these airports' flights are. Similar to the most frequented locations, in the graphs below we see the locations with the most airports still have larger circles, but lighter colors represent longer delays on average for the year. We know from previous graphs the length of delays is somewhat seasonal, but on average we see that the more frequented airports do not have relatively long delays. We actually notice in states like Iowa there is not a lot of flights from here but they have a significantly higher average length of delay. We see something similar for flights arriving in Tennessee. 
```{r,fig.align='center',echo=FALSE}
# Average Arrival Delays per Airport
arrivalDealysPerAirport <- abia %>% filter(Origin != 'AUS') %>% 
  select(Origin, Origin_Longitude, Origin_Latitude, ArrDelay, Density_Arrivals) %>% 
  group_by(Origin) %>% 
  summarize('longitude' = mean(Origin_Longitude, na.rm = TRUE),
            'latitude' = mean(Origin_Latitude, na.rm = TRUE),
            'Arrival_Delay' = mean(ArrDelay, na.rm = TRUE),
            'Number_of_Flights' = mean(Density_Arrivals, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=arrivalDealysPerAirport, aes(x=longitude, y=latitude, color=Arrival_Delay, size=Number_of_Flights))

# Average Departure Delays per Airport
depatureDealysPerAirport <- abia %>% filter(Dest != 'AUS') %>% 
  select(Dest, Dest_Longitude, Dest_Latitude, DepDelay, Density_Destination) %>% 
  group_by(Dest) %>% 
  summarize('longitude' = mean(Dest_Longitude, na.rm = TRUE),
            'latitude' = mean(Dest_Latitude, na.rm = TRUE),
            'Departure_Delay' = mean(DepDelay, na.rm = TRUE),
            'Number_of_Flights' = mean(Density_Destination, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=depatureDealysPerAirport, aes(x=longitude, y=latitude, color=Departure_Delay, size=Number_of_Flights))


########## Add Jenny's Graphs#######################################
```

# Question 3
# Question 4
```{r}

```
# Question 5
```{r}

```
# Question 6 
Initial exploration of the grocery dataset reveals that the top three most frequently purchased items are whole milk, other vegetables, and rolls/buns.  The top twenty most frequently purchased items are shown in the figure below.  The list includes common pantry and fridge staples you would expect to see in most grocery carts, such as milk, dairy products, fruit and vegetables, soda and beer, bread, juice, and eggs.   
```{r,include=FALSE}
library(tinytex)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(png)
library(arules)
library(arulesViz)

# read transactions
groceries <- read.transactions("groceries.txt", format = "basket", sep = ",")
groceries_df <- as(groceries, "data.frame") 
# see dimensions
dim(groceries)
# see basic summary
summary(groceries)
# see the first 10 transactions
arules::inspect(groceries[1:10])
# see the number of items in first 20 transactions
size(groceries[1:20])
# see list of item labels
itemLabels(groceries)
```
```{r,fig.align='center',echo=FALSE}
# plot most frequently sold items
itemFrequencyGGPlot <- function(x, topN) {
  x %>%
    itemFrequency %>%
    sort %>%
    tail(topN) %>%
    as.data.frame %>%
    tibble::rownames_to_column() %>%
    ggplot(aes(reorder(rowname, `.`),`.`)) + 
    theme_bw() +
    geom_col(fill="#F05483") + 
    coord_flip() +
    labs(y="Frequency", x ="Item", title = "Most Frequently Purchased Grocery Items") +
    theme(plot.title = element_text(hjust = 0.5))
}  

itemFrequencyGGPlot(groceries, topN=20)
```
On the other end, the least frequently purchased items include a lot of non-food items, including toiletries (hairspray, makeup remover), cleaning and organization products (rubbing alcohol, toilet cleaner, bags), and baby products (baby food and baby cosmetics).  
```{r,,fig.align='center',echo=FALSE}
# plot least purchased items
itemFrequencyGGPlot <- function(x, topN) {
  x %>%
    itemFrequency %>%
    sort %>%
    head(topN) %>%
    as.data.frame %>%
    tibble::rownames_to_column() %>%
    ggplot(aes(reorder(rowname, `.`),`.`)) + 
    theme_bw() +
    geom_col(fill="#FFAAC4") + 
    coord_flip() +
    labs(y="Frequency", x ="Item", title = "Least Frequently Purchased Grocery Items") +
    theme(plot.title = element_text(hjust = 0.5))
}  

itemFrequencyGGPlot(groceries, topN=20)
```
Looking at the association rules with the highest support, we see that some of the most popular items are also the items most frequently purchased together, which makes sense.  For example, whole milk and other vegetables were purchased together in 7.5 percent percent of all transactions, the highest of any pair of items.  In fact, the majority of rules with high support include whole milk or other vegetables.  The other two combinations of items included in at least 5percent of shoppers’ carts include whole milk and rolls/buns, and whole milk with yogurt.  As one would expect, the rules with the highest support were those that involved only two items, with support decreasing and confidence increasing as the order of magnitude went up.
```{r,include=FALSE}
##########################################################
# APRIORI
##########################################################

# Which groups of items are most frequently purchased together? (highest support)
# apriori alogrithm (support 5%)
grocRules <- apriori(groceries, parameter = list(supp = 0.05, conf = .0001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame") 
grocRulesDF[order(grocRulesDF$support, decreasing=TRUE), ] 

# Which groups of items have the highest lift?
# apriori alogrithm
grocRules <- apriori(groceries, parameter = list(supp = 0.001, conf = .001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame") 
grocRulesDF[order(grocRulesDF$lift, decreasing=TRUE)[0:20], ] 

# Interesting rules - which ones have higher confidence and higher lift
# apriori alogrithm
grocRulesInt <- apriori(groceries, parameter = list(supp = 0.001, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt,subset = lift > 2,by = 'support',decreasing = T)))

# Interesting rules - without whole milk and other vegetables?
# apriori alogrithm
grocRulesInt2 <- apriori(groceries, parameter = list(supp = 0.0015, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt2,subset=!(rhs %in% c('whole milk','other vegetables')) & lift > 3,by = 'support',decreasing = T)))
```
```{r,fig.align='center',echo=FALSE}
plot(grocRules, method='two-key plot')

```
When looking for interesting rules, we set a confidence minimum of 0.5 (in order to find rules that were correct for at least half of transactions containing the combination of items).  Since a lift of 1 implies that the pair of items are independent of one another, we also initially filtered on a lift value of 2 to give a minimum confidence that was at least twice what we would expect to see from items that were independent from one another.  A lot of the rules generated had whole milk and other vegetables as the antecedents, which makes sense given that they are the top two selling items.  To get an idea of other interesting rules that involved less popular items, we then filtered out rules that had those two products as antecedents.  

After applying these filters, there were still a lot of rules.  To narrow these down further, additional tuning was applied: support was set to 0.0015 and the lift was increased to 3.  The resulting list included some expected rules and others that were surprising.  For example, there are a lot of rules that included root or other vegetables with things like herbs, beef, butter, rice, onions, rolls - all things that are typically dinner mainstays and often cooked together as well.  One rule was that liquor and red wine are associated with bottled beer, and this rule had a lower support but comparatively high confidence and lift (90 percent confidence, lift of 11), which makes sense - all three items are usually purchased if someone is throwing a party, but parties themselves are not a weekly occurrence for most people.  Some other party pairings also had higher lifts, including instant food products/soda/hamburger meat and popcorn/soda/salty snacks.  This information could be used to put together end caps with common party foods or to offer coupons targeted at party hosters.
```{r,fig.align='center',echo=FALSE}
plot(grocRulesInt2, measure = c("support", "lift"), shading = "confidence")

```


There were also some interesting rules that were less intuitive.  One such combination included cream cheese, root vegetables, and yogurt (found in 38 transactions).  Yet another surprising combination included beef, waffles, and root vegetables.  Another rule demonstrated that curd and tropical fruit were purchased with yogurt in 52 transactions, and 51 percent of the time when the first two were purchased so was yogurt.  With a lift of 3.69, we can be almost 4 times as confident that a customer will purchase yogurt knowing that they purchased curd and tropical fruit.  After researching, it appears those three items can be used to make homemade flavored yogurt as well as certain types of desserts, so it makes sense that they would be purchased together, but it is surprising that the combination appears as frequently as it does.  Based on this insight, one recommendation could be to set up a tasting/demonstration station that shows customers how to create their own yogurt and offers samples for them to try.

After graphing the top 1k interesting rules to show degree centrality, betweenness, and modularity, it was interesting to see that whole milk (the most frequently purchased item and one that appears in many rules) had a much lower degree of betweenness than other products such as yogurt, fruit, and vegetables.  
![Grocery Network.](grocerynetwork.png)





