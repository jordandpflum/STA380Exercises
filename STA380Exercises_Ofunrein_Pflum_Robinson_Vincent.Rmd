---
title: "Predictive Modeling Exercises"
author: "Kessiena Ofunrein, Jordan Pflum, Jennifer Robinson, and Katelyn Vincent"
date: "8/8/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
header-includes:
- \usepackage{placeins}
- \usepackage{tikz}
- \usetikzlibrary{mindmap,shadows}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=5.5, fig.height=3)
```
# Question 1 
```{r,include=FALSE}
library(dplyr)
library(magrittr)
# get rid of scientific notation
options(scipen=999)

# read in data set
greenData <- read.csv("greenbuildings.csv")

# separate into green and non-green buildings
greenBuildings <- greenData %>% filter(green_rating == 1)
nonGreenBuildings <- greenData %>% filter(green_rating == 0)

# create subset of similar buildings based on what we know:
# mixed use building so look at buildings with amenities = 1
# new building so filter on buildings <10 yrs or renovated
# 15 stories so filter on buildings between 10-20 stories
# Austin, TX so filter on buildings with num cooling days > median of 966

subset_All <- greenData %>% filter(amenities==1&
                                   cd_total_07>=966&
                                   stories<=20&
                                   stories>=10&
                                   age<=10 | renovated==1)

subset_Green <- subset_All %>% filter(green_rating==1)
subset_Nongreen <- subset_All %>% filter(green_rating==0)
```
Though potentially accurate in their findings, the conclusions of the initial analysis regarding the economic viability of “going green” were based on flawed logic.  The employee filtered out buildings with occupancy rates of less than ten percent, assuming that the occupancy rate for the new building will be over ten percent without providing evidence to support his theory.  To strengthen his argument, he could have used a visual like the one below, which shows the leasing rates by size for all of the buildings in the data set.  Based on this, it appears that there are in fact no ‘green’ buildings in the data set with leasing rates (aka occupancy rates) lower than 10 percent. 
```{r,fig.align='center',echo=FALSE}
#####################################################
# plot leasing rate vs size, all data
#####################################################
library(ggplot2)
# plot leasing rate vs size (all data)
ggplot(data = greenData) + 
  geom_point(aes(x = leasing_rate, y = size, color=factor(green_rating))) +
  labs(color = "Green Building") +
  # add colors and legend labels
  scale_colour_manual(values = c("#01148A","#0AD626"),labels = c("No","Yes")) +
  ggtitle("Leasing Rate vs. Size") +
  # center plot title
  theme(plot.title = element_text(hjust = 0.5))+
  labs(x = "Leasing Rate",y = "Size (Sq. ft)")
```
While the idea of filtering the dataset is a good one, the filtering approach did not take into account the information that is known about the new project - that it will be a 15-story mixed-use building in Austin, TX measuring 250,000 sq. ft.  This information, though limited, allows us to narrow our focus to buildings with similar properties.  For example, we created a subset of the data that contained information about buildings that: were between 10-20 stories, were either less than 10 years old or had been renovated, had amenities (since the building will be mixed-use), and had annual cooling degree days over 966 (the median for the whole dataset).  Since there were no variables directly related to location, we used high cooling degree days to filter on locations in similar climates to Austin, TX (if not similar geographic regions).  This subset contained 407 buildings, including 33 designated as ‘green buildings’.  
Subsetting the data yields different information about the rent of green vs non-green buildings.  Below is the rent distribution for all buildings in the dataset, which would seem to support the original analysis.

```{r,fig.align='center',echo=FALSE}
#####################################################
# distribution of rental prices, all buildings
#####################################################

plot(density(greenBuildings$Rent), main="Rent Distribution, All Buildings", ylim=c(0,.045), col = "#0AD626", xlab = "Rent Per Square Foot")
lines(density(nonGreenBuildings$Rent),col = "#01148A")
legend(100, .035, legend=c("Green", "Not Green"),
       col=c("#0AD626", "#01148A"), lty=1:2, cex=0.8)
```
However, when analyzing green buildings in the subset of data (containing information on buildings similar to the new one), it appears that the rent distribution skews slightly more towards less expensive rental prices.  

```{r,fig.align='center',echo=FALSE}
# green buildings compared (all to subset)
plot(density(greenBuildings$Rent), main="Rent Distribution, Green Buildings", ylim=c(0,.06), xlim=c(0,100), col = "#69E878", xlab = "Rent Per Square Foot")
lines(density(subset_Green$Rent),col = "#155B2F")
legend(70, .05, legend=c("All Data", "Subset"),
       col=c("#69E878", "#155B2F"), lty=1:2, cex=0.8)
```
The median rent for green buildings in the subset is 24.36 dollars vs 22.50 dollars, this is still higher, but both are significantly less than the rental prices in the original analysis.  
```{r,fig.align='center',echo=FALSE}
#####################################################
# plot dist. of rent prices for subset
#####################################################
green.denplot <- function(df,TITLE){
  attach(df)
  median.column = round(median(df$Rent),digits=2)
  graph = df %>%
    ggplot(aes(x=Rent))+
    geom_density(fill="#0AD626",alpha=0.5)+
    scale_x_log10()+
    geom_vline(xintercept = median.column,size=1,color="black")+
    geom_label(aes(x = median.column, y = 0.5, label = paste("Median Rent: $", median.column)), fill = "white")+
    labs(y="Density", x ="Rent Per Sq.Ft.", title = TITLE) +
    theme(plot.title = element_text(hjust = 0.5))
  return(graph) 
}

green.denplot(subset_Green, "Rent Distribution (Subset, Green Buildings)")

nongreen.denplot <- function(df,TITLE){
  attach(df)
  median.column = round(median(df$Rent),digits=2)
  graph = df %>%
    ggplot(aes(x=Rent))+
    geom_density(fill="#01148A",alpha=0.5)+
    scale_x_log10()+
    geom_vline(xintercept = median.column,size=1,color="black")+
    geom_label(aes(x = median.column, y = 0.5, label = paste("Median Rent: $", median.column)), fill = "white")+
    labs(y="Density", x ="Rent Per Sq.Ft.", title = TITLE) +
    theme(plot.title = element_text(hjust = 0.5))
  return(graph) 
}

nongreen.denplot(subset_Nongreen, "Rent Distribution (Subset, Non-Green Buildings)")
```
Additionally, while the original analysis factored in expected revenue differences, it failed to account for differences in electricity cost savings over the year from “going green”.  Only three percent of all buildings (and six percent of green buildings) in the original dataset have rent quoted on a “net contract” basis, where the tenant pays for utility costs, which means that the majority of utility costs are paid by the building owners.  
Estimated annual electricity costs for a non-green building were computed using the building size (250,000 sq ft), avg number of kWh per square foot for commercial buildings (22.5), and median electricity costs for buildings in the subset.  The annual electricity costs for the green building assumed a 25 percent reduction in energy usage.  To compute revenue, the median occupancy rate was multiplied by the median rent (for green and non-green buildings in the subset, separately).  Below is the break-even analysis for a new green building, which takes into account the difference in costs and expected revenues compared to the construction of a non-green building.  Based on this analysis, the additional costs of the green building would be recouped in 9.5 years.
```{r,fig.align='center',echo=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=5)

#####################################################
# plot break even analysis
#####################################################

# fixed costs
fixed_cost <- 5000000

# annual electricity costs
# average kwh for commercial buildings is 22.5
# https://www.iotacommunications.com/blog/benchmarking-commercial-building-energy-use-per-square-foot/#:~:text=According%20to%20the%20Department%20of,are%20consumed%20by%20refrigeration%20%26%20equipment.
annual_kwh_persqf <- 22.5
building_size_sqf <- 250000
elec_cost <- median(subset_All$Electricity_Costs)
ann_elec_usage_other <- annual_kwh_persqf*building_size_sqf
ann_elec_costs_other <- ann_elec_usage_other*elec_cost
# average 25% reduction in energy usage for green buildings
# https://www.iotacommunications.com/blog/green-building-benefits/
ann_elec_usage_green <- ann_elec_usage_other*0.75
ann_elec_costs_green <- ann_elec_usage_green*elec_cost
delta_elec_costs <- ann_elec_costs_green - ann_elec_costs_other

# annual revenue
occ_rate_other <- median(subset_Nongreen$leasing_rate)/100
rent_other <- median(subset_Nongreen$Rent)
ann_revenue_other <- building_size_sqf*occ_rate_other*rent_other
occ_rate_green <- median(subset_Green$leasing_rate)/100
rent_green <- median(subset_Green$Rent)
ann_revenue_green <- building_size_sqf*occ_rate_green*rent_green
delta_revenue <- ann_revenue_green - ann_revenue_other

years <- c(seq(0, 15))
fixed_cost <- c(rep(5000000, length(years)))
variable_cost <- delta_elec_costs*years
total_cost <- fixed_cost + variable_cost
revenue <- delta_revenue*years
contribution_margin <- delta_revenue - delta_elec_costs
break_even_years <- round(fixed_cost[1]/contribution_margin,digits=1)
breakeven <- data.frame(years, fixed_cost, variable_cost, total_cost, revenue)

# plot break-even analysis
ggplot(data = breakeven, aes(x = years)) +
  geom_line(aes(y = fixed_cost,
                col = "Fixed Cost")) +
  geom_line(aes(y = variable_cost,
                col = "Variable Cost")) +
  geom_line(aes(y = total_cost,
                col = "Total Cost")) +
  geom_line(aes(y = revenue,
                col = "Revenue")) +
  geom_segment(aes(x = break_even_years, xend = break_even_years, 
                   y = 0, yend = break_even_years*delta_revenue),
               linetype = "dashed") +
  geom_segment(aes(x = 0, xend = break_even_years, 
                   y = break_even_years*delta_revenue, yend = break_even_years*delta_revenue),
               linetype = "dashed") +
  geom_point(aes(x = break_even_years, 
                 y= break_even_years*delta_revenue), 
             colour = "black", size = 4) +
  annotate("text", x = break_even_years, y = 0, label = paste("Break Even Point:", break_even_years,"years")) +
  scale_color_manual(labels = c("Fixed Cost", "Delta Revenue", "Total Cost", "Delta Variable Cost"),
                     values = c("Fixed Cost" = "black", "Variable Cost" = "blue", "Total Cost" = "red", "Revenue" = "green")) +
  geom_ribbon(data = breakeven[breakeven$total_cost >= breakeven$revenue, ], aes(x = years, ymin = revenue, ymax = total_cost), fill = "red", alpha = 0.15) +
  geom_ribbon(data = breakeven[breakeven$total_cost <= breakeven$revenue, ], aes(x = years, ymin = total_cost, ymax = revenue), fill = "green", alpha = 0.15) +
  labs(title = "Break-Even Analysis (Green Building)",
       x = "Year",
       y = "Dollars",
       color = NULL) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_y_continuous(labels=scales::dollar_format())
```
One possible confounding variable for the relationship between rent and green status in the initial analysis could be the building class.  Almost 80% of all green buildings are Class A buildings, which are the highest quality buildings in a market, whereas only 36 percent of non-green buildings have a Class A designation.  Since Class A buildings command a much higher rent than other building types, what appeared to be a higher rent for ‘green’ buildings could be attributed to the fact that most of them are of a higher quality building type.  
```{r,fig.align='center',echo=FALSE}
knitr::opts_chunk$set(fig.width=5.5, fig.height=3)

#####################################################
# plot rent distribution for class a buildings (all)
#####################################################

classABuildings <- greenData %>% filter(class_a == 1)
otherClassBuildings <- greenData %>% filter(class_a == 0)

classa.denplot <- function(df,TITLE){
  attach(df)
  median.column = round(median(df$Rent),digits=2)
  graph = df %>%
    ggplot(aes(x=Rent))+
    geom_density(fill="#45CCE4",alpha=0.5)+
    scale_x_log10()+
    geom_vline(xintercept = median.column,size=1,color="black")+
    geom_label(aes(x = median.column, y = 0.5, label = paste("Median Rent: $", median.column)), fill = "white")+
    labs(y="Density", x ="Rent Per Sq.Ft.", title = TITLE) +
    theme(plot.title = element_text(hjust = 0.5))
  return(graph) 
}

classa.denplot(classABuildings, "Rent Distribution (All Class A Buildings)")
classa.denplot(otherClassBuildings, "Rent Distribution (All Non-Class A Buildings)")

```

# Question 2 
The figures below portray a story about the flights arriving in and departing from the Austin Bergstrom International Airport in 2008. We looked at traits of flights and airlines from a few different perspectives to convey an accurate image of what the airport and travelers experienced that year. This analysis of the data is extremely useful for a few different potential clients. Firstly, travelers are the sustenance of this industry and are impacted by the factors we are analyzing. By providing them with some of these graphs, travelers will be more informed about some things they are likely to experience temporally, geographically, or with a specific airline. Secondly, airport officials would benefit greatly from our analysis because it is advantageous to know what one is likely to experience with each airport. Airports are businesses with reputations and standards. Notoriety for delayed flights is very useful information for preparations and strategic planning. 
```{r,include=FALSE}
library(magrittr)
library(ggplot2)
library(ggmosaic)
library(varhandle)
library(lubridate)
library(dplyr)
#################################   DATE COLUMN & MORE ###########################
#set up the data frame and graphs
#FILES NEEDED IN FOLDER:
    #ABIA.csv
    #airline_codes.csv
###############################################################################3
# add column for airline
airport <- read.csv("ABIA.csv",header=T, na.strings=c("",NA))
airlinecodes <- read.csv("airline_codes.csv")
abia <- airport %>% left_join(airlinecodes,by="UniqueCarrier")

# replace n/a cells with zero
abia[is.na(abia)] <- 0
abia[abia=='NULL'] <- 0

# recode cancellation code
abia$CancellationCode <- recode(abia$CancellationCode, 
                                "A"="Carrier",
                                "B"="Weather",
                                "C"="NAS")

abia$CancellationCode <- factor(abia$CancellationCode, 
                                levels= c("Carrier",
                                          "Weather",
                                          "NAS"))

# recode days of week
abia$DayOfWeek <- recode(abia$DayOfWeek, 
                         "7"="Sunday",
                         "1"="Monday",
                         "2"="Tuesday",
                         "3"="Wednesday",
                         "4"="Thursday",
                         "5"="Friday",
                         "6"="Saturday")

abia$DayOfWeek <- factor(abia$DayOfWeek, 
                         levels= c("Monday", 
                                   "Tuesday",
                                   "Wednesday", 
                                   "Thursday",
                                   "Friday", 
                                   "Saturday", 
                                   "Sunday"))

# recode months
abia$MonthName <- recode(abia$Month, 
                         "1"="Jan",
                         "2"="Feb",
                         "3"="Mar",
                         "4"="April",
                         "5"="May",
                         "6"="June",
                         "7"="July",
                         "8"="Aug",
                         "9"="Sept",
                         "10"="Oct",
                         "11"="Nov",
                         "12"="Dec")

abia$MonthName <- factor(abia$MonthName, 
                         levels= c("Jan",
                                   "Feb",
                                   "Mar",
                                   "April",
                                   "May",
                                   "June",
                                   "July",
                                   "Aug",
                                   "Sept",
                                   "Oct",
                                   "Nov",
                                   "Dec"))

# create date column
abia$date <- as.Date(with(abia, paste(Year, Month, DayofMonth,sep="-")), "%Y-%m-%d")
abia$weeknum <- isoweek(abia$date)

# set colorblind-friendly palette
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
###############################################################################
#SUBSET                   most popular airlines (above mean)
commonair <- data.frame(count(abia, vars = Airline)) #df of airlines and frequency in abia data frame
airline.counts <- table(abia$Airline) #table of counts similar to commonair
barplot(airline.counts, main="Frequency of Airlines",
        xlab="Airline")

airline.mean <- mean(commonair$n) #mean frequency of airlines in abia
x.sub <- commonair %>%
  filter(n >= airline.mean) #data frame of list of airlines who's frequency is > median
airlines <- x.sub$vars #list of popular airlines

popular.abia <- subset(abia, Airline %in% airlines) #data frame with info for each flight for popular airlines
###############################################################################
#SUBSET                   FILTER OUT SHORT DELAYS AND MOST POPULAR AIRLINES AIRLINES~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``
summary(popular.abia$ArrDelay)
m<- mean(popular.abia$ArrDelay, na.rm=TRUE) #exclude NA from calculations 
longARRdelays <-popular.abia %>% filter(ArrDelay > m)

summary(abia$DepDelay)
n<- mean(popular.abia$DepDelay, na.rm=TRUE) #exclude NA from calculations 
longDEPdelays <-popular.abia %>% filter(DepDelay > n)
```
The line graph figures below are a time series analysis of how the length of flight delays changes throughout the year 2018. The graphs are relatively similar and have peaks during similar times. They are separated by arriving and departing flights, essentially whether the plane was late coming in or late coming out. It is to be expected that a change in one would impact an airport’s schedule thus resulting in changes in the other. For both graphs there are peaks in months like March, June, and December. It is easy to deduce the cause of that could be holidays or related to school schedules like Spring Break. For a more concrete picture further analysis should be conducted. The airlines graphed here have more flights than the average airline and the delays are considered long delays because they are in the 75th percentile. 
```{r,fig.align='center', echo=FALSE} 
##############################    LINE GRAPHS   ##############################

#LINE PLOT MONTH VS AVERAGE ARRIVAL DELAY TIME BY AIRLINE FOR POPULAR AIRLINES -----------------------------DONE 
AvgArrDelay <- longARRdelays %>%
  group_by(Month, Airline) %>%
  summarize(avgArrDelay = mean(ArrDelay, na.rm = TRUE))
ggplot(AvgArrDelay, aes(x=Month, y=avgArrDelay, group=Airline))+ 
  scale_fill_brewer(palette="Paired") +
  geom_line(aes(color=Airline))+
  geom_point(aes(color=Airline))+ 
  ggtitle("Average Arrival Delay Over Time (Year:2018)") +
  xlab("Airline") + 
  ylab("Average Arrival Delay")
#---------------------------------------------------------------------------------------END

#LINE PLOT MONTH VS AVERAGE DEPARTURE DELAY TIME BY AIRLINE FOR POPULAR AIRLINES -----------------------------DONE 
AvgDepDelay <- longARRdelays %>%
  group_by(Month, Airline) %>%
  summarize(avgDepDelay = mean(DepDelay, na.rm = TRUE))
ggplot(AvgDepDelay, aes(x=Month, y=avgDepDelay, group=Airline))+ 
  scale_fill_brewer(palette="Paired") +
  geom_line(aes(color=Airline))+
  geom_point(aes(color=Airline))+ 
  ggtitle("Average Departure Delay Over Time (Year:2018)") +
  xlab("Airline") + 
  ylab("Average Departure Delay")
#---------------------------------------------------------------------------------------END
```
As we transition our focus from how flights are delayed for longer periods of time seasonally, we can also look at the different types of cancellations and delays. The graph below enables us to compare three types of cancellation codes: Carrier, Weather, and NAS. This plot is clearly able to show that NAS delays do not account for many delays and truly do not compare to weather or carrier delays. 
```{r,fig.align='center', echo=FALSE}
# aggregate the data set by more than one factor
d2 = abia  %>% filter(abia$CancellationCode != 0) %>%
  group_by(CancellationCode, DayOfWeek) %>%
  summarize(numcancels=n()/52)

# plot
ggplot(data = d2) + 
  geom_bar(mapping = aes(x=DayOfWeek, y=numcancels, fill=CancellationCode),
           position="dodge", stat='identity') +
  labs(x = "Weekday", fill = "Cancellation Code") +
  scale_fill_manual(values=cbPalette) +
  theme_bw() +
  ggtitle("Average Number of Cancelled Flights by Weekday in 2008") +
  # center plot title
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = "Weekday",y = "Avg # Cancelled Flights")
```
In the graph below, we look at a similar trend but over months so we can look at a seasonal picture of the flights. Some interesting anomalies are found in April and September. Based on knowledge of seasonal changes in weather, one could conclude that the significant flux in weather delays in September could be explained by hurricane season. As for April, carrier delays could increased significantly because American Airlines canceled a lot of flights due to not meeting government regulation standards. 
```{r,fig.align='center',echo=FALSE}
# aggregate the data set by more than one factor
d12 = abia %>% filter(abia$CancellationCode != 0) %>%
  group_by(CancellationCode, MonthName) %>%
  summarize(numcancelled=n())

d13 = abia %>%
  group_by(MonthName) %>%
  summarize(numflights=n(),.groups = 'drop') %>%
  ungroup() %>%
  arrange(desc(numflights))

combo4 <-merge(x=d12,y=d13,by="MonthName",all.x=TRUE)
combo4$cancperc <- combo4$numcancelled/combo4$numflights
combo4$numcancelled <- NULL
combo4$numflights <- NULL

# plot
ggplot(data = combo4) + 
  geom_bar(mapping = aes(x=MonthName, y=cancperc, fill=CancellationCode),
           position="dodge", stat='identity') +
  labs(x = "Month", fill = "Cancellation Code") +
  scale_fill_manual(values=cbPalette) +
  theme_bw() +
  ggtitle("Percent of Flights Cancelled by Month in 2008") +
  # center plot title
  theme(plot.title = element_text(hjust = 0.5))+
  # add labels
  labs(x = "2008",y = "% Flights Cancelled")

```
For personal planning, I would be helpful to know what time of day there are long delays or irrgularities with flights. We used our data to generate the graph below and interpret whether time of day is truly significant in planning. These graphs are a picture of all states as well as arriving and departing flights so there is likely a lot of noise, but it is still helpful to see the occasions where there are abnormally long flight delays. The histogram below shows the number of delayed flights during the average day while the scatter plot shows how long these delays are relative to time of day. 
```{r,include=FALSE}
library(mosaic)
library(dplyr)
library(readr)
airport <- read.csv("ABIA.csv",header=T, na.strings=c("",NA))
airlinecodes <- read.csv("airline_codes.csv")
abia <- airport %>% left_join(airlinecodes,by="UniqueCarrier")

```
```{r,fig.align='center',echo=FALSE}
d3 = abia  %>% filter(DepDelay >=30) %>%
  group_by(CRSDepTime) %>%
  summarize(Frequency=n())


qplot(d3$CRSDepTime,
      geom="histogram",
      binwidth = 50,  
      main = "Delay Frequency Based on Time of Day", 
      xlab = "Time of Day (hhmm)", 
      ylab = "# of Delays",
      fill=I('blue'),
      col=I("black"),
      alpha=I(.5))
d4 = abia  %>% filter(DepDelay >=30) %>%
  group_by(CRSDepTime) %>%
  summarize(DepDelay)

d4.glm <- glm(cbind(DepDelay, 875 - DepDelay) ~ CRSDepTime, 
                    data=d4, family=binomial(logit))

plot(DepDelay ~ CRSDepTime, data=d4, main='Depature Delay Length Based on Time of Day', xlab='Time of Day (hhmm0', ylab='Departure Delay Time')

```
Strategic planning is a fundamental component of how airports sustain themselves. Knowing that an airport or region is notorious for delays or that they are notorious for this seasonally, plans can be made to accommodate to yield the best options for travelers. A simple way to summarize this information in a plot is through geographical mapping. Figure ___ shows this; airports with larger volumes of flights arriving in or departing from Austin have larger and brighter circles. An airport official should show more concern for a large dot that is lightly shaded, as this airport has a lot of flights outgoing to the Austin airport so schedules in these airports are integral to an efficient airport.
```{r,include=FALSE}
library(splitstackshape)
library(ggmap)
# Load Data
abia <- read.csv('ABIA.csv')
airportCodes <- read.csv('airportCodes.csv')

# Rename Airport Codes iata_code and get unique codes
abiaDestination <- abia %>% select(Dest) %>% rename('iata_code' = Dest) %>% distinct()
abiaOrgin <- abia %>% select(Origin) %>% rename('iata_code' = Origin) %>% distinct()

# Get destination codes
abiaDestination <- merge(abiaDestination, airportCodes, by='iata_code') %>% 
  distinct(iata_code, .keep_all= TRUE) %>% 
  select(iata_code, coordinates) 

# Get Origin Codes
abiaOrgin <- merge(abiaOrgin, airportCodes, by='iata_code') %>% 
  distinct(iata_code, .keep_all= TRUE) %>% 
  select(iata_code, coordinates)

# Split into Longitude and Latitude (Destination)
abiaDestination <- cSplit(abiaDestination, 'coordinates', sep=", ", type.convert=FALSE) %>% 
  rename('latitude' = coordinates_1,
         'longitude' = coordinates_2
  )
abiaDestination$latitude <- as.numeric(abiaDestination$latitude)
abiaDestination$longitude <- as.numeric(abiaDestination$longitude)

# Split into Longitude and Latitude (Orgin)
abiaOrgin <- cSplit(abiaOrgin, 'coordinates', sep=", ", type.convert=FALSE) %>% 
  rename('latitude' = coordinates_1,
         'longitude' = coordinates_2
  )
abiaOrgin$latitude <- as.numeric(abiaOrgin$latitude)
abiaOrgin$longitude <- as.numeric(abiaOrgin$longitude)


# Merge Dataframe back to original dataframe (Desitnation)
abiaDestination <- abiaDestination %>% rename('Dest' = iata_code)
abia <- merge(abia, abiaDestination, by='Dest') %>% rename('Dest_Longitude' = longitude,
                                                                       'Dest_Latitude' = latitude)

# Merge Dataframe back to original dataframe (Orgina)
abiaOrgin <- abiaOrgin %>% rename('Origin' = iata_code)
abia <- merge(abia, abiaOrgin, by='Origin') %>% rename('Origin_Longitude' = longitude,
                                                                   'Origin_Latitude' = latitude)
# Get Density of Departures
densityOfDepatrues <- abia %>% count(Dest)
abia <- merge(abia, densityOfDepatrues, by='Dest') %>% rename('Density_Destination' = n)

# Get Density of Arrivals
densityOfArrivals <- abia %>% count(Origin)
abia <- merge(abia, densityOfArrivals, by='Origin') %>% rename('Density_Arrivals' = n)
```

```{r,fig.align='center',include=FALSE}
# Mapping
register_google(key = "AIzaSyDjF7I4JjprxlUM4ZFB_0AScyEME5HsI-s")
```
```{r,fig.align='center',echo=FALSE}
##############################    GEO PLOT   ##############################
#######################    DEPARTURES PER AIRPORT   ########################

usa <- c(left = -125, bottom = 25.75, right = -67, top = 49)
map <- get_stamenmap(bbox = usa, zoom = 4, maptype = "toner-lite")


# Number of Depatures per Airport
numDepatures <- abia %>% filter(Dest != 'AUS') %>% 
  select(Dest, Dest_Longitude, Dest_Latitude, Density_Destination) %>% 
  group_by(Dest) %>% 
  summarize('longitude' = mean(Dest_Longitude, na.rm = TRUE),
            'latitude' = mean(Dest_Latitude, na.rm = TRUE),
            'Number_Flight_Depatures' = mean(Density_Destination, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=numDepatures, aes(x=longitude, y=latitude, color=Number_Flight_Depatures, size=Number_Flight_Depatures))

# Number of Arrivals per Airport
numArrivals <- abia %>% filter(Origin != 'AUS') %>% 
  select(Origin, Origin_Longitude, Origin_Latitude, Density_Arrivals) %>% 
  group_by(Origin) %>% 
  summarize('longitude' = mean(Origin_Longitude, na.rm = TRUE),
            'latitude' = mean(Origin_Latitude, na.rm = TRUE),
            'Number_Flight_Arrivals' = mean(Density_Arrivals, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=numArrivals, aes(x=longitude, y=latitude, color=Number_Flight_Arrivals, size=Number_Flight_Arrivals))

```
Now that we have an understanding of where the most frequented airports are, we can now look at a similar map that tells us how timely these airports' flights are. Similar to the most frequented locations, in the graphs below we see the locations with the most airports still have larger circles, but lighter colors represent longer delays on average for the year. We know from previous graphs the length of delays is somewhat seasonal, but on average we see that the more frequented airports do not have relatively long delays. We actually notice in states like Iowa there is not a lot of flights from here but they have a significantly higher average length of delay. We see something similar for flights arriving in Tennessee. 
```{r,fig.align='center',echo=FALSE}
# Average Arrival Delays per Airport
arrivalDealysPerAirport <- abia %>% filter(Origin != 'AUS') %>% 
  select(Origin, Origin_Longitude, Origin_Latitude, ArrDelay, Density_Arrivals) %>% 
  group_by(Origin) %>% 
  summarize('longitude' = mean(Origin_Longitude, na.rm = TRUE),
            'latitude' = mean(Origin_Latitude, na.rm = TRUE),
            'Arrival_Delay' = mean(ArrDelay, na.rm = TRUE),
            'Number_of_Flights' = mean(Density_Arrivals, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=arrivalDealysPerAirport, aes(x=longitude, y=latitude, color=Arrival_Delay, size=Number_of_Flights))

# Average Departure Delays per Airport
depatureDealysPerAirport <- abia %>% filter(Dest != 'AUS') %>% 
  select(Dest, Dest_Longitude, Dest_Latitude, DepDelay, Density_Destination) %>% 
  group_by(Dest) %>% 
  summarize('longitude' = mean(Dest_Longitude, na.rm = TRUE),
            'latitude' = mean(Dest_Latitude, na.rm = TRUE),
            'Departure_Delay' = mean(DepDelay, na.rm = TRUE),
            'Number_of_Flights' = mean(Density_Destination, na.rm = TRUE)
  )
ggmap(map) + geom_point(data=depatureDealysPerAirport, aes(x=longitude, y=latitude, color=Departure_Delay, size=Number_of_Flights))


########## Add Jenny's Graphs#######################################
```

# Question 3
```{r,include=FALSE}
############ Setup ############ 
#### Libraries ####
library(quantmod)
library(timeSeries)
library(fPortfolio)
library(PerformanceAnalytics)
library(ggplot2)


# Bootstrapping
library(mosaic)

# Optimization
library(PortfolioAnalytics)
library(DEoptim)
library(ROI)
require(ROI.plugin.glpk)
require(ROI.plugin.quadprog)

#### Get Returns ####
# Portfolio
tickers <- c("AGG", "GLD", "VUG")

# Get AdjClose prices
adjClosePrices <- NULL
for (ticker in tickers){
  adjClosePrices <- cbind(adjClosePrices,
                          quantmod::getSymbols(ticker, from="2015-08-01", to = '2020-08-01', verbose=FALSE, auto.assign=FALSE)[,6])
}

# keep only the dates that have closing prices for all tickers
adjClosePrices <- adjClosePrices[apply(adjClosePrices,1,function(x) all(!is.na(x))),]

# Convert AdjClose to Returns
returns <- Return.calculate(adjClosePrices)[-1,1:length(tickers)]

############ Construct Portfolios ############ 
#### Portfolio 1 ####
port1_weights <- c(0.8, 0.15, 0.05)

# Create portfoli (Rebalancing Daily) 
port1_returns <- Return.portfolio(returns, weights = port1_weights, rebalance_on = "days")

#### Portfolio 2 ####
port2_weights <- c(0.1, 0.2, 0.7)

# Create portfoli (Rebalancing Daily) 
port2_returns <- Return.portfolio(returns, weights = port2_weights, rebalance_on = "days")

#### Portfolio 3 ####
portfolioAssets <- colnames(returns)
init <- portfolio.spec(assets=portfolioAssets)
# Add Constraint (Weights sum to 1, ie full investment)
init <- add.constraint(portfolio=init,
                       type="weight_sum",
                       min_sum=1,
                       max_sum=1)

# Add Constraint (No shorting or borrowing, and no over/under investment)
init <- add.constraint(portfolio=init,
                       type="box",
                       min=0.05,
                       max=0.5
)

# Add Constraint (Target Return = 10% annual, 0.00055% daily)
init <- add.constraint(portfolio=init, type="return", return_target=0.00055)

# Add Objective (Minimize risk)
minvar <- add.objective(portfolio=init,
                        type='risk',
                        name='var'
)

# Optimize Portfolio
opt_maxret <- optimize.portfolio(R=returns, portfolio=minvar,
                                 optimize_method="ROI",
                                 trace=FALSE
)

# Create portfoli (Rebalancing Daily) 
port3_returns <- Return.portfolio(returns, weights = opt_maxret$weights, rebalance_on = "days")


#### Bench Mark Portfolio ####
tickers <- c("SPY")

# Get AdjClose prices
adjClosePrices <- NULL
for (ticker in tickers){
  adjClosePrices <- cbind(adjClosePrices,
                          quantmod::getSymbols(ticker, from="2015-08-01", to = '2020-08-01', verbose=FALSE, auto.assign=FALSE)[,6])
}

# keep only the dates that have closing prices for all tickers
adjClosePrices <- adjClosePrices[apply(adjClosePrices,1,function(x) all(!is.na(x))),]

# Convert AdjClose to Returns
#returns <- as.timeSeries((tail(adjClosePrices,-1) / as.numeric(head(adjClosePrices,-1)))-1)
market_returns <- Return.calculate(adjClosePrices)[-1,]
```
Our group chose to construct a simple portfolio of three ETFs: iShares Core U.S. Aggregate Bond ETF (AGG), SPDR Gold Shares (GLD), Vanguard Growth Index Fund ETF Shares (VUG). We chose AGG because it is considered a relatively safe investment and would help mitigate the volatility of VUG. Additionally, we selected GLD, an ETF which tracks the price of Gold (about 1/10) as Gold has been a hot commodity in the wake of COVID-19 and the expected rise in inflation rates. Figure 1 depicts the Cumulative Return, Daily Return, and Drawdown of each of the assets.
```{r,fig.align='center',echo=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=5)

charts.PerformanceSummary(returns,main='Portfolio Assets Summary', rf=.0071)
```
The top chart unsurprisingly shows that VUG has the highest cumulative return over the past five years (as it is the most risky investment of the three) and AGG as the lowest. However, it is also apparent that GLD has high returns in the past 6 months and nears the cumulative return of VUG. The bottom graph depicts the drawdown for every asset during each period. The drawdown of an asset refers to how much an investment or trading account is down from the peak before it recovers back to the peak. Therefore, it is generally a good measure of the negative risk of an asset.
```{r,fig.align='center',echo=FALSE}
knitr::opts_chunk$set(fig.width=5.5, fig.height=3)

chart.RiskReturnScatter(returns, add.sharpe = TRUE, Rf=.0071)
```
Figure 1 does not do the best job of displaying the return of an asset in respect to the risk taken on. Therefore, Figure 2 plots the annualized return for every asset vs annualized risk (SD). You can see that VUG has the highest risk of all three assets but the also the highest return. One last graph that is useful to examine is each of the assets relative performance to the market. To simplify things, we used SPY, and ETF that track the S and P500, as our market proxy. Figure 3 shows each of the assets performance relative to SPY. If SPY were graphed, it would be a horizontal line intersection the y-axis at 1. Therefore, whenever an asset is above 1, they are performing better than the market.
```{r,fig.align='center',echo=FALSE}
chart.RelativePerformance(apply.weekly(returns, Return.cumulative), apply.weekly(market_returns, Return.cumulative), legend.loc='topright')

```
We can see that VUG has consistently been beating the market while the less risky assets GLD and AGG have not, although GLD has made a recent push above the market.

These three figures confirm our initial thoughts of the risk-reward ratio of each of the assets and why we chose them to comprise our portfolio. However, we then needed to decide how much to invest in each of the assets in our portfolio. 

We decided for our first portfolio, Portfolio 1, we would create a very safe portfolio to satisfy a client who was risk-averse. Therefore, we invested heavily in AGG (0.8), lightly in GLD (0.15) and almost nothing in VUG (0.05). 

For our second portfolio we decided to invest risky, perhaps for a young client with a lot of new wealth and in the position to take on financial risk. For this portfolio, Portfolio 2, we invested heavily in VUG (0.7), moderately in GLD (0.2) and lightly in AGG (0.1). 

Lastly, we constructed our third portfolio by optimizing our portfolio (in respect to Variance)  subject to a set of constraints:

1. Full Investment: $\sum_{i}w_i = 1, \forall$ portfolios
2. No Over/Under Investment: $0.05 \leq w_i \leq 0.5 \forall$ portfolios
3. Target Return (about 10 percent annual): $w' \bar\mu = 0.00055$
Our objective was to minimize the variance of the portfolio given the constraints listed above. This resulted in a portfolio with:

* $w_{AAG} = 0.0659$
* $w_{GLD} = 0.500$
* $w_{VUG} = 0.4341$
A summary of our portfolio weights can be seen in Table 1.

![Table 1](shot1.png)

The portfolios constructed above yielded the following numbers (monthly) (Table2):

![Table 2](shot2.png)
The table does a nice job of summarizing the portfolios. Portfolio 1, which is the safest of the three, has the lowest return and the lowest VaR. However, although Portfolio 2 has a higher return than portfolio 3, it has a much higher risk and VaR than Portfolio 3 and therefore a considerably lower Sharpe Ratio. Because Portfolio 3 was optimized, it had a much better relative performance than Portfolio 2.
```{r,fig.align='center',echo=FALSE}
############ VaR Calculation ############
#### Conceptual ####
conceptualCaR <- function(portfolio, confidence){
  portfolio_resampled <- mosaic::resample(portfolio)
  portfolioReturns <- coredata(portfolio_resampled$portfolio.returns)
  sortedPortfolioReturns <- sort(portfolioReturns, decreasing=FALSE)
  port_dailyVar <- quantile(sortedPortfolioReturns,c(confidence))
  port_monthlyVaR <- port_dailyVar*sqrt(20)
  return(port_monthlyVaR)
}

# Analysis (Port 1)
# Bootstrap
monthlyVaR_bootstrap = do(1000)*conceptualCaR(port1_returns,.05)
port1_montlyVaR <- mean(monthlyVaR_bootstrap$X5.)

# Analysis (Port 2)
# Bootstrap
monthlyVaR_bootstrap = do(1000)*conceptualCaR(port2_returns,.05)
port2_montlyVaR <- mean(monthlyVaR_bootstrap$X5.)


# Analysis (Port 3)
# Bootstrap
monthlyVaR_bootstrap = do(1000)*conceptualCaR(port3_returns,.05)
port3_montlyVaR <- mean(monthlyVaR_bootstrap$X5.)

#### Normal ####
# Analysis (Port 1)
# Bootstrap
dailyVaR_bootstrap = do(1000)*PerformanceAnalytics::VaR(mosaic::resample(port1_returns))
port1_montlyVaR <- mean(dailyVaR_bootstrap$VaR)*sqrt(20)

# Analysis (Port 2)
# Bootstrap
dailyVaR_bootstrap = do(1000)*PerformanceAnalytics::VaR(mosaic::resample(port2_returns))
port2_montlyVaR <- mean(dailyVaR_bootstrap$VaR)*sqrt(20)


# Analysis (Port 3)
# Bootstrap
dailyVaR_bootstrap = do(1000)*PerformanceAnalytics::VaR(mosaic::resample(port3_returns))
port3_montlyVaR <- mean(dailyVaR_bootstrap$VaR)*sqrt(20)




#### Graphing ####
# Returns
port1_monthly = apply.weekly(port1_returns, Return.cumulative)
port1 <- data.frame(port1_monthly)
port1$portfolio <- 'Portfolio 1'

port2_monthly = apply.weekly(port2_returns, Return.cumulative)
port2 <- data.frame(port2_monthly)
port2$portfolio <- 'Portfolio 2'

port3_monthly = apply.weekly(port3_returns, Return.cumulative)
port3 <- data.frame(port3_monthly)
port3$portfolio <- 'Portfolio 3'

portfolioReturns <- rbind(port1, port2, port3)

# Portfolio VaRs
port1_VaR <- data.frame(PortfolioVaR=port1_montlyVaR)
port1_VaR$portfolio <- 'Portfolio 1'

port2_VaR <- data.frame(PortfolioVaR=port2_montlyVaR)
port2_VaR$portfolio <- 'Portfolio 2'

port3_VaR <- data.frame(PortfolioVaR=port3_montlyVaR)
port3_VaR$portfolio <- 'Portfolio 3'

portfolioVaRs <- rbind(port1_VaR, port2_VaR, port3_VaR)


ggplot(portfolioReturns, aes(portfolio.returns, fill = portfolio)) + 
  geom_density(alpha = 0.2) + 
  geom_vline(data=portfolioVaRs, aes(xintercept=PortfolioVaR, color = portfolio), linetype="dashed") + 
  ggtitle("Historical Returns (Monthly) vs VaR") +
  xlab("Returns")
```
Figure 4 does a good job of illustrating the table information. With little risk, Portfolio 1’s distribution is skinny and high, indicating narrow but consistent returns. However, although Portfolio 2’s and 3’s returns are very similar, Portfolio 2’s risk and VaR are a lot lower. This can most noticeably be seen in the levels of VaR (plotted as dashed vertical lines). The interpretation of the VaR line is simple: 
* VaR(95) = -4.033 Implies your portfolio has a 5 percent chance of losing 4.033 percent or more in a given month.
It is also useful to translate our portfolio to dollars. Let's assume that we have 100,000 dollars to invest in our portfolio. Table 3 shows the monthly Expected Return, Risk, and VaR of each portfolio. For example, in any given month, our portfolio (3) would be expected to grow on average 1,105 dollars. We would be 95 percent confident that it would grow within the range of -5057 dollars and 7,267 dollars. Finally, we would expect there to be a 5 percent chance that we would lose 4,022 dollars in any given month.
![Table 3](shot3.png)




# Question 4
After performing K-means clustering on the Social Marketing dataset and subsetting the customers into 6 segments, we got the following top categories for each segment:
Segment 1: Sports fandom, religion, and food
	The members of this group are likely adults with families.
Segment 2: College, online gaming, photo sharing (excluding chatter)
	This customer segment likely represents college students or teenagers.
Segment 3: Cooking, fashion, photo sharing
	This group majority is most likely young to middle-aged women.
Segment 4: Politics, news, travel
	This customer group likely represents educated, middle-aged or older men and women.
Segment 5: Chatter, photo sharing, current events (including chatter because it is top rated)
	This group likely contains teenagers and young adults.
Segment 6: Health nutrition, personal fitness, cooking (excluding chatter)
	The members of this group are clearly invested in health and fitness lifestyles.

In addition to performing K-Means clustering, we also implemented PCA to see if any more information could be gathered about classifying the twitter users.

```{r, include=FALSE}
social_marketing <- read.csv('social_marketing.csv')

social_marketing_filtered <- social_marketing %>% filter(adult<=5) %>% dplyr::select(-spam)
rownames(social_marketing_filtered) <- social_marketing_filtered$X


## find percentages of tweet types by each user
total_col = apply(social_marketing_filtered[,-1], 1, sum)
pcts = lapply(social_marketing_filtered[,-1], function(x) {
  x / total_col
})

pcts$X = social_marketing_filtered$X

pcts = as.data.frame(pcts)


#k-means cluster

X = pcts[ , !(names(pcts) %in% c('X'))]


X = scale(X, center=TRUE, scale=TRUE)
clusters <- kmeans(X, 6, nstart=25)

mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

cluster1 <- clusters$center[1,]*sigma + mu
cluster2 <- clusters$center[2,]*sigma + mu
cluster3 <- clusters$center[3,]*sigma + mu
cluster4 <- clusters$center[4,]*sigma + mu
cluster5 <- clusters$center[5,]*sigma + mu
cluster6 <- clusters$center[6,]*sigma + mu

#sort each cluster's values
cluster1 <- as.data.frame(cluster1)
cluster1$categories <- row.names(cluster1)
cluster1[order(cluster1$cluster1, decreasing = TRUE),]

cluster2 <- as.data.frame(cluster2)
cluster2$categories <- row.names(cluster2)
cluster2[order(cluster2$cluster2, decreasing = TRUE),]

cluster3 <- as.data.frame(cluster3)
cluster3$categories <- row.names(cluster3)
cluster3[order(cluster3$cluster3, decreasing = TRUE),]

cluster4 <- as.data.frame(cluster4)
cluster4$categories <- row.names(cluster4)
cluster4[order(cluster4$cluster4, decreasing = TRUE),]

cluster5 <- as.data.frame(cluster5)
cluster5$categories <- row.names(cluster5)
cluster5[order(cluster5$cluster5, decreasing = TRUE),]

cluster6 <- as.data.frame(cluster6)
cluster6$categories <- row.names(cluster6)
cluster6[order(cluster6$cluster6, decreasing = TRUE),]
```
![Bar Chart](barchart.png)
Because the first 10 principal components were only able to capture 73 percent of the variation, we decided to keep all 10 principal components. However, the first two principal components were the most interesting. Thus, we constructed a correlation circle to analyze the relationship of variable between the first two principal components.
![Scatter Plot](scatter.png)
Although many variable have little to no correlation with each other, a few stand out. We can see that politics and College_uni are negatively correlated with personal fitness, nutrition, aand outdoors. Therefore, we could speculate that users who primarily tweet about politics and college uni are generally adverse to nutrition information of personal fitness tweets. 

Principal Components turned out not to be the best method for categorizing tweets. As seen in the plot of the first 10 PC and their cos2 values (quality of representation of variables on factor map), most of the variables do not score highly. A low cos2 value would mean that the variable is not perfectly represented in their respective Principal Component. The graph shows that in the first principal component, only chatter and health nutrition seem to be important. The same goes for the second principal component. Therefore, the use of Principal Components in this specific dataset is not particularly helpful.
![Correlation Matrix](cor.png)

# Question 5
The goal of the project was to build a predictive model that accurately identifies the author of a given article.  The data used in the analysis includes a training folder containing 50 articles from each of 50 different authors, as well as a testing folder with a similar number of articles from the same authors.  The information in the training folder was used to train the models, which were then used to predict authors in the testing folders.  

The original data was in the form of txt files (one for each article), which were grouped together in folders by author.  In order to make this data usable, we wrote a function to read in one txt file per line, which gave us a matrix of 2500 rows, with each cell containing the entirety of one article.  We then created a dataframe for the training and testing sets, each containing roughly 2500 rows and two columns for author name and article.  An additional column was added to denote the article number (1-2500).  

With the goal of doing text analysis on the articles, each article was split into individual words, resulting in a dataframe that had columns for article number, author, word, and the number of times that word appeared in the article.  Believing that sentiment analysis could be useful for predicting an author, we also utilized the Afinn library to assign sentiment values to each word.  The Afinn lexicon assigns words with a score from -5 to 5, with -5 indicating strong negative sentiment and 5 indicating strong positive sentiment.  A column with sentiment value was added to the existing dataframe broken out by word count.  

The data frame was then grouped by article number, author, and word, and transposed using melt and dcast in order to set the word counts as columns that could then be used as variables.  The column headers for the resulting data frame were the article number, author, and individual words (eg “word_abandoned”, “word_abducted”) and the total count of instances of each word in a given article.  Since there were missing values after this restructuring, they were replaced with counts of zero.

We then utilized the sentiment scores to determine the total positive and negative ‘scores’ for each article, which was calculated by multiplying the sentiment score of a word by the number of instances of that word in an article.  All of the positive scores for an article were then added together, as well as the negative scores, to create two new columns.  For example, if an article contained the word ‘love’ twice and ‘happiness’ once, and the sentiment score for those two words were 5 and 3, then the positive score for that article would be (2 instances of ‘love’ x 5 score) + (1 instance of ‘happiness’ x 3 score) = 13 total positive score.    

Taking into account the ‘new’ words that would be seen in the testing data, we took the existing dataframe (which had almost 1500 columns) and counted the total number of instances of each word (across all articles).  We then filtered the training data to include only the top 1000 words by count (in addition to the article number, author, and total positive and negative scores).  An additional column was added with a tally of the ‘other words’ - those that were not in the top 1000 words.  This ‘other words’ bucket could then be utilized for the testing set to account for words in those articles that did not fall into the top 1000 words from the testing set.  

PCA was a logical choice to reduce dimensionality, as there were still just over 1000 columns after all of the data cleaning and data wrangling.  The values in the dataset were normalized before running PCA.  Interestingly, the first principal component appeared to be divided into observations that were marked by high counts of achievement words on one end (eg ‘growth’, ‘boost’, ‘competitive’, ‘assets’) and high counts of crime words on the other end (eg ‘crime’, ‘detained’, ‘penalty’, ‘sentenced’, ‘harsh’).  

After running PCA, various classification models were trained on the data, including logistic regression, decision tree, random forest, gradient boosting (GBM), and kNN. 
```{r}

library(tm)
library(topicmodels)
library(caret)
library(ggplot2)
library(keras)
library(reshape2)
library(readtext)
library(tidyverse)
library(glmnet)
library(tidytext)
library(dplyr)
library(tibble)
library(textdata)

########################################################################
# READ IN FILES
########################################################################

setwd("C:/Users/katel/OneDrive/Desktop/QUESTION5/C50train")

# read in txt files for training set
a <- 1
txtfile <- list()
X_train <- list()
y_train <- list()
tmp = list.files(pattern="*.*")

for (i in 1: length(tmp)) {for (x in 1:50)
{ y_train[a] <- tmp[i]
  a <- a+1}
  author <- tmp[i]
  files <- paste0("C:/Users/katel/OneDrive/Desktop/QUESTION5/C50train/", author, "/*")
  nextauth <- readtext(files)
  X_train <- rbind(X_train, nextauth)}
X_train <- X_train$text

# read in txt files for testing set
a <- 1
txtfile <- list()
X_test <- list()
y_test <- list()
tmp = list.files(pattern="*.*")

for (i in 1: length(tmp)) {for (x in 1:50)
{ y_test[a] <- tmp[i]
a <- a+1}
  author <- tmp[i]
  files <- paste0("C:/Users/katel/OneDrive/Desktop/QUESTION5/C50test/", author, "/*")
  nextauth <- readtext(files)
  X_test <- rbind(X_test, nextauth)}
X_test <- X_test$text

########################################################################
# PREPROCESSING - TRAIN SET
########################################################################

# create and export csv file for train info
df_train <- data.frame(matrix(unlist(y_train), nrow=length(y_train), byrow=T))
author_train <- cbind(df_train,X_train)
colnames(author_train) <- c("author", "article")
write.csv(author_train,"C:/Users/katel/OneDrive/Desktop/QUESTION5/author_train.csv", row.names = FALSE)

# create article number column
author_train$articlenum <- 1:nrow(author_train)

# get word counts for individual words (minus fillers)
# get sentiment scores for words (negative or positive: values from -5 to 5)
afinn <- get_sentiments("afinn")
train_a <- unnest_tokens(author_train, input = article, output = word) %>%
  anti_join(stop_words) %>% 
  count(articlenum, author, word, sort = TRUE) %>%
  inner_join(afinn, by = "word")

# transpose word count columns to create variables based on particular word counts
train_b <- train_a %>% group_by(articlenum,author,word=word) %>% 
  summarise(word_=sum(n)) %>%
  melt(id.var=c("articlenum","author","word")) %>%
  dcast(articlenum + author ~ variable + word)

# fill n/a values with 0
train_b[is.na(train_b)] <- 0

# create positive and negative score columns that add up the total sentiment
# for example: if 'cut' has a score of -1 and is used 16 times, the negative
# score column would have a value of 16
train_a$positive_score <- ifelse(train_a$n*train_a$value >=0, train_a$n*train_a$value, 0)
train_a$negative_score <- ifelse(train_a$n*train_a$value <0, -1*(train_a$n*train_a$value), 0)

avgsentiment <-  train_a %>% group_by(articlenum) %>% 
  summarise(tot_pos_score = sum(positive_score), 
            tot_neg_score = sum(negative_score))

trainfinal <- merge(train_b,avgsentiment,by="articlenum")
trainfinal <- trainfinal[,c(1,2,1484,1485,3:1483)]
y_train_final <- trainfinal[,c('author')]
X_train_almost <- trainfinal[,!(colnames(trainfinal) %in% c('author', 'articlenum'))]

# rearrange columns based on column sums
xtrain_sorted <- X_train_almost[,order(colSums(-X_train_almost,na.rm=TRUE))]
# create a dataframe with pos/neg scores and top 1000 used words
top1k_words <- xtrain_sorted %>% select(1:1002)
keepcols <- colnames(top1k_words)
# create a dataframe with all other words (not in the top 1000)
idx <- match(keepcols, names(xtrain_sorted))
other_words <- xtrain_sorted[,-idx] 
other_words_total <- rowSums(other_words)

X_train_final <- cbind(top1k_words,other_words_total)

########################################################################
# PREPROCESSING - TEST SET
########################################################################

# create and export csv file for test info
df_test <- data.frame(matrix(unlist(y_test), nrow=length(y_test), byrow=T))
author_test <- cbind(df_test,X_test)
colnames(author_test) <- c("author", "article")
write.csv(author_test,"C:/Users/katel/OneDrive/Desktop/QUESTION5/author_test.csv", row.names = FALSE)

# create article number column
author_test$articlenum <- 1:nrow(author_test)

# get word counts for individual words (minus fillers)
# get sentiment scores for words (negative or positive: values from -5 to 5)
afinn <- get_sentiments("afinn")
test_a <- unnest_tokens(author_test, input = article, output = word) %>%
  anti_join(stop_words) %>% 
  count(articlenum, author, word, sort = TRUE) %>%
  inner_join(afinn, by = "word")

# transpose word count columns to create variables based on particular word counts
test_b <- test_a %>% group_by(articlenum,author,word=word) %>% 
  summarise(word_=sum(n)) %>%
  melt(id.var=c("articlenum","author","word")) %>%
  dcast(articlenum + author ~ variable + word)

# fill n/a values with 0
test_b[is.na(test_b)] <- 0

# create positive and negative score columns that add up the total sentiment
# for example: if 'cut' has a score of -1 and is used 16 times, the negative
# score column would have a value of 16
test_a$positive_score <- ifelse(test_a$n*test_a$value >=0, test_a$n*test_a$value, 0)
test_a$negative_score <- ifelse(test_a$n*test_a$value <0, -1*(test_a$n*test_a$value), 0)

avgsentiment <-  test_a %>% group_by(articlenum) %>% 
  summarise(tot_pos_score = sum(positive_score), 
            tot_neg_score = sum(negative_score))

testfinal <- merge(test_b,avgsentiment,by="articlenum")
testfinal <- testfinal[,c(1,2,1534,1535,3:1533)]

y_test_final <- testfinal[,c('author')]
X_test_almost <- testfinal[,!(colnames(testfinal) %in% c('author', 'articlenum'))]

# create a dataframe with all other words (not in the top 1000)
idx_test <- match(keepcols, names(X_test_almost))
idx_use <- idx_test[!is.na(idx_test)]
missingfromtest <- which(is.na(idx_test))
xtest_keepfromtrain <- X_test_almost[,idx_use]

keepcolsdf <- enframe(keepcols)
missingfromtestdf <- enframe(missingfromtest)
main_data2 <- keepcolsdf[keepcolsdf$name %in% missingfromtestdf$value, ]

avector <- as.vector(main_data2$value)
for(i in avector)
  xtest_keepfromtrain[,i] <- NA

xtest_keepfromtrain[is.na(xtest_keepfromtrain)] <- 0

other_words_test <- X_test_almost[,-idx_use] 
other_words_total <- rowSums(other_words_test)
X_test_almostfinal <- cbind(xtest_keepfromtrain,other_words_total)

library(data.table)

X_test_final <- setcolorder(X_test_almostfinal, c(keepcols))
X_test_final <- X_test_final[,!(names(X_test_final) %in% avector)]
X_train_final <- X_train_final[,!(names(X_train_final) %in% avector)]

########################################################################
# PCA - training
########################################################################

# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z = X_train_final/rowSums(X_train_final)

# PCA
pc2 = prcomp(Z, scale=TRUE, rank=5)
loadings = pc2$rotation
scores = pc2$x

# How are the individual PCs loaded on the original variables?
# The top words associated with each component

# PC1 = looks to be associated with achievement words (eg gains, increase, stronger, boosted)
# opposite end are punishments/harsh things (eg penalty, detained, accused, guilty)
o1 = order(loadings[,1], decreasing=TRUE)
colnames(Z)[head(o1,25)]
colnames(Z)[tail(o1,25)]

# PC2
o2 = order(loadings[,2], decreasing=TRUE)
colnames(Z)[head(o2,25)]
colnames(Z)[tail(o2,25)]

########################################################################
# PCA - testing
########################################################################

# First normalize phrase counts to phrase frequencies.
# (often a sensible first step for count data, before z-scoring)
Z_test = X_test_final/rowSums(X_test_final)

# PCA
pc_test = prcomp(Z_test, scale=TRUE, rank=5)
loadings_test = pc_test$rotation
scores_test = pc_test$x

# How are the individual PCs loaded on the original variables?
# The top words associated with each component

# PC1 = looks to be associated with achievement words (eg gains, increase, stronger, boosted)
# opposite end are punishments/harsh things (eg penalty, detained, accused, guilty)
o1_test = order(loadings_test[,1], decreasing=TRUE)
colnames(Z_test)[head(o1_test,25)]
colnames(Z_test)[tail(o1_test,25)]

# PC2
o2_test = order(loadings_test[,2], decreasing=TRUE)
colnames(Z_test)[head(o2_test,25)]
colnames(Z_test)[tail(o2_test,25)]

########################################################################
# MODELING
########################################################################

# Look at the data, call caret package and normalization package
library(caret)
library(BBmisc)
library(kernlab)
library(openxlsx)

# Model 1 (Logistic Regression)
set.seed(123)
mode(scores) = "numeric"
scorex <- data.frame(scores)
training <- cbind(y_train_final, scorex)
training$y_train_final <- factor(training$y_train_final)

scores_test <- data.frame(scores_test)
testing <- cbind(y_test_final, scores_test)
testing$y_test_final <- factor(testing$y_test_final)

logr.fit = glm(y_train_final~., data = training, 
              family = binomial)
```
# Question 6 
Initial exploration of the grocery dataset reveals that the top three most frequently purchased items are whole milk, other vegetables, and rolls/buns.  The top twenty most frequently purchased items are shown in the figure below.  The list includes common pantry and fridge staples you would expect to see in most grocery carts, such as milk, dairy products, fruit and vegetables, soda and beer, bread, juice, and eggs.   
```{r,include=FALSE}
library(tinytex)
library(ggplot2)
library(tidyverse)
library(dplyr)
library(png)
library(arules)
library(arulesViz)

# read transactions
groceries <- read.transactions("groceries.txt", format = "basket", sep = ",")
groceries_df <- as(groceries, "data.frame") 
# see dimensions
dim(groceries)
# see basic summary
summary(groceries)
# see the first 10 transactions
arules::inspect(groceries[1:10])
# see the number of items in first 20 transactions
size(groceries[1:20])
# see list of item labels
itemLabels(groceries)
```
```{r,fig.align='center',echo=FALSE}
# plot most frequently sold items
itemFrequencyGGPlot <- function(x, topN) {
  x %>%
    itemFrequency %>%
    sort %>%
    tail(topN) %>%
    as.data.frame %>%
    tibble::rownames_to_column() %>%
    ggplot(aes(reorder(rowname, `.`),`.`)) + 
    theme_bw() +
    geom_col(fill="#F05483") + 
    coord_flip() +
    labs(y="Frequency", x ="Item", title = "Most Frequently Purchased Grocery Items") +
    theme(plot.title = element_text(hjust = 0.5))
}  

itemFrequencyGGPlot(groceries, topN=20)
```
On the other end, the least frequently purchased items include a lot of non-food items, including toiletries (hairspray, makeup remover), cleaning and organization products (rubbing alcohol, toilet cleaner, bags), and baby products (baby food and baby cosmetics).  
```{r,fig.align='center',echo=FALSE}
# plot least purchased items
itemFrequencyGGPlot <- function(x, topN) {
  x %>%
    itemFrequency %>%
    sort %>%
    head(topN) %>%
    as.data.frame %>%
    tibble::rownames_to_column() %>%
    ggplot(aes(reorder(rowname, `.`),`.`)) + 
    theme_bw() +
    geom_col(fill="#FFAAC4") + 
    coord_flip() +
    labs(y="Frequency", x ="Item", title = "Least Frequently Purchased Grocery Items") +
    theme(plot.title = element_text(hjust = 0.5))
}  

itemFrequencyGGPlot(groceries, topN=20)
```
Looking at the association rules with the highest support, we see that some of the most popular items are also the items most frequently purchased together, which makes sense.  For example, whole milk and other vegetables were purchased together in 7.5 percent percent of all transactions, the highest of any pair of items.  In fact, the majority of rules with high support include whole milk or other vegetables.  The other two combinations of items included in at least 5percent of shoppers’ carts include whole milk and rolls/buns, and whole milk with yogurt.  As one would expect, the rules with the highest support were those that involved only two items, with support decreasing and confidence increasing as the order of magnitude went up.
```{r,include=FALSE}
##########################################################
# APRIORI
##########################################################

# Which groups of items are most frequently purchased together? (highest support)
# apriori alogrithm (support 5%)
grocRules <- apriori(groceries, parameter = list(supp = 0.05, conf = .0001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame") 
grocRulesDF[order(grocRulesDF$support, decreasing=TRUE), ] 

# Which groups of items have the highest lift?
# apriori alogrithm
grocRules <- apriori(groceries, parameter = list(supp = 0.001, conf = .001, minlen=2, maxlen=5))
grocRulesDF <- as(grocRules, "data.frame") 
grocRulesDF[order(grocRulesDF$lift, decreasing=TRUE)[0:20], ] 

# Interesting rules - which ones have higher confidence and higher lift
# apriori alogrithm
grocRulesInt <- apriori(groceries, parameter = list(supp = 0.001, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt,subset = lift > 2,by = 'support',decreasing = T)))

# Interesting rules - without whole milk and other vegetables?
# apriori alogrithm
grocRulesInt2 <- apriori(groceries, parameter = list(supp = 0.0015, conf = .5, minlen=2, maxlen=5))
inspect(sort(subset(grocRulesInt2,subset=!(rhs %in% c('whole milk','other vegetables')) & lift > 3,by = 'support',decreasing = T)))
```
```{r,fig.align='center',echo=FALSE}
plot(grocRules, method='two-key plot')

```
When looking for interesting rules, we set a confidence minimum of 0.5 (in order to find rules that were correct for at least half of transactions containing the combination of items).  Since a lift of 1 implies that the pair of items are independent of one another, we also initially filtered on a lift value of 2 to give a minimum confidence that was at least twice what we would expect to see from items that were independent from one another.  A lot of the rules generated had whole milk and other vegetables as the antecedents, which makes sense given that they are the top two selling items.  To get an idea of other interesting rules that involved less popular items, we then filtered out rules that had those two products as antecedents.  

After applying these filters, there were still a lot of rules.  To narrow these down further, additional tuning was applied: support was set to 0.0015 and the lift was increased to 3.  The resulting list included some expected rules and others that were surprising.  For example, there are a lot of rules that included root or other vegetables with things like herbs, beef, butter, rice, onions, rolls - all things that are typically dinner mainstays and often cooked together as well.  One rule was that liquor and red wine are associated with bottled beer, and this rule had a lower support but comparatively high confidence and lift (90 percent confidence, lift of 11), which makes sense - all three items are usually purchased if someone is throwing a party, but parties themselves are not a weekly occurrence for most people.  Some other party pairings also had higher lifts, including instant food products/soda/hamburger meat and popcorn/soda/salty snacks.  This information could be used to put together end caps with common party foods or to offer coupons targeted at party hosters.
```{r,fig.align='center',echo=FALSE}
plot(grocRulesInt2, measure = c("support", "lift"), shading = "confidence")

```


There were also some interesting rules that were less intuitive.  One such combination included cream cheese, root vegetables, and yogurt (found in 38 transactions).  Yet another surprising combination included beef, waffles, and root vegetables.  Another rule demonstrated that curd and tropical fruit were purchased with yogurt in 52 transactions, and 51 percent of the time when the first two were purchased so was yogurt.  With a lift of 3.69, we can be almost 4 times as confident that a customer will purchase yogurt knowing that they purchased curd and tropical fruit.  After researching, it appears those three items can be used to make homemade flavored yogurt as well as certain types of desserts, so it makes sense that they would be purchased together, but it is surprising that the combination appears as frequently as it does.  Based on this insight, one recommendation could be to set up a tasting/demonstration station that shows customers how to create their own yogurt and offers samples for them to try.

After graphing the top 1k interesting rules to show degree centrality, betweenness, and modularity, it was interesting to see that whole milk (the most frequently purchased item and one that appears in many rules) had a much lower degree of betweenness than other products such as yogurt, fruit, and vegetables.  
![Grocery Network.](grocerynetwork.png)





